{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import data_processing as dp\n",
    "import json\n",
    "\n",
    "import t2g\n",
    "# import g2t\n",
    "import g2t_copy as g2t\n",
    "\n",
    "# # instantiate models\n",
    "# g2t_model = SimpleT5()\n",
    "# t2g_model = t2g.T2GModel()\n",
    "\n",
    "# # load (supports t5, mt5, byT5 models)\n",
    "# g2t_model.from_pretrained(\"t5\",\"t5-base\")\n",
    "\n",
    "\n",
    "#create dataloader\n",
    "#t, g = dp.create_cycle_dataloader(vocab, batch_size = 8, shuffle=True)\n",
    "\n",
    "\n",
    "class CycleModel():\n",
    "\tdef __init__(self, vocab, device = \"cpu\"):\n",
    "\t\tif device == \"gpu\":\n",
    "\t\t\tself.device = torch.device('cuda:0')\n",
    "\t\telse:\n",
    "\t\t\tself.device = torch.device('cpu')\n",
    "\n",
    "\t\tself.t2g_model = t2g.T2GModel(vocab, self.device, 768)\n",
    "\t\tself.g2t_model = g2t.G2TModel(vocab)\n",
    "\t\tself.t2g_opt = torch.optim.Adam(self.t2g_model.model.parameters())\n",
    "\t\tself.g2t_opt = torch.optim.Adam(self.g2t_model.t5_model.parameters())\n",
    "\t\tself.vocab = vocab\n",
    "    \n",
    "\tdef t_cycle(self, text_batch): # optimizes g2t\n",
    "\t\tself.t2g_model.eval()\n",
    "\t\tself.g2t_model.train()\n",
    "\n",
    "\t\tgold_text = self.g2t_model.g2t_preprocess(text_batch, mode=\"TGT\")\n",
    "\t\t#print(gold_text)\n",
    "\t\t# gold_text = gold_text.to(self.device) # bs x gold_text_len\n",
    "\t\t# bs, gold_text_len = gold_text.shape\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred_graphs = self.t2g_model.predict(text_batch) #synthetic batch of graphs\n",
    "\t\t\n",
    "\t\tself.g2t_opt.zero_grad()\n",
    "\n",
    "\t\t# #tokenize pred_graphs\n",
    "\t\tpred_graphs, ents, raw_ents = self.g2t_model.g2t_preprocess(pred_graphs, mode=\"G2T\")\n",
    "\t\tpred_graphs_ids = self.g2t_model.tokenizer(pred_graphs, return_tensors='pt', truncation=True, padding=True).input_ids\n",
    "\t\tgold_text_ids = self.g2t_model.tokenizer(gold_text, return_tensors='pt', truncation=True, padding=True).input_ids\n",
    "\t\tloss = self.g2t_model.t5_model(input_ids = pred_graphs_ids, labels = gold_text_ids).loss\n",
    "\t\tloss.backward()\n",
    "\t\tself.g2t_opt.step()\n",
    "\t\treturn loss.item()\n",
    "\n",
    "\n",
    "\tdef g_cycle(self, graph_batch): # optimizes t2g\n",
    "\t\t\"\"\"\n",
    "\t\t\tInput: graph_batch: list (length batch_size) of dicts with entities and relations\n",
    "\n",
    "\t\t\tPerforms G2T then optimizes T2G by computing loss of generated graph and original (gold) graph\n",
    "\t\t\"\"\"\n",
    "\t\tself.g2t_model.eval()\n",
    "\t\tself.t2g_model.train()\n",
    "\t\tmax_ents = max([len(graph[\"entities\"]) for graph in graph_batch])\n",
    "\t\tgold_graphs = [dp.relation2Indices(self.vocab, graph, max_ents) for graph in graph_batch]\n",
    "\t\tgold_graphs = torch.stack(gold_graphs)\n",
    "\t\tgold_graphs = gold_graphs.to(self.device) # bs x max_ents x max_ents - used for loss computation\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred_text = self.g2t_model.predict(graph_batch)\n",
    "\n",
    "\t\t#print(\"pred_text\", pred_text)\n",
    "\t\t#print(gold_graphs[0])\n",
    "\t\t#print(pred_text[0])\n",
    "\n",
    "\t\tself.t2g_opt.zero_grad()\n",
    "\t\t\n",
    "\t\tpred_text, pred_text_ents = self.t2g_model.t2g_preprocess(pred_text)\n",
    "\n",
    "\t\t# print(\"important\", max([max([ent_ind[0] for ent_ind in batch_ent_inds])] for batch_ent_inds in pred_text_ents)[0].item() + 1)\n",
    "\n",
    "\t\t# print(\"post_pred_text\", pred_text)\n",
    "\t\t# print(\"ent_inds\", pred_text_ents)\n",
    "\n",
    "\t\t#graph_log_probs = self.t2g_model.model.forward(pred_text.to(self.device), pred_text_ents.to(self.device)) # bs x max_ents x max_ents x num_relations - log probs of each relation between all entities in each batch\n",
    "\t\tgraph_log_probs = self.t2g_model.model.forward(pred_text.to(self.device), pred_text_ents.to(self.device), torch.tensor(max_ents).to(self.device)) # bs x max_ents x max_ents x num_relations - log probs of each relation between all entities in each batch\n",
    "\n",
    "\t\t# print(graph_log_probs.shape)\n",
    "\t\t# print(gold_graphs.shape)\n",
    "\t\t\n",
    "\t\tloss = F.nll_loss(graph_log_probs.view(-1, graph_log_probs.shape[-1]), gold_graphs.view(-1), ignore_index=self.vocab.relations.word2idx['<EMPTY>']) # ignore index should be 0\n",
    "\t\tloss.backward()\n",
    "\t\t#nn.utils.clip_grad_norm_(g2t_model.parameters(), config['clip'])\n",
    "\t\tself.t2g_opt.step()\n",
    "\t\treturn loss.item()\n",
    "\n",
    "\tdef back_translation(self, text_batch, graph_batch):\n",
    "\t\tg_loss = self.g_cycle(graph_batch)\n",
    "\t\tt_loss = self.t_cycle(text_batch)\n",
    "\t\treturn g_loss, t_loss\n",
    "\n",
    "\tdef train(self, epochs, batch_size, learning_rate, shuffle):\n",
    "\n",
    "\t\tfor i in range(epochs):\n",
    "\t\t\ttcycle_dataloader, gcycle_dataloader = dp.create_cycle_dataloader(raw_json_file=self.vocab.raw_data, batch_size = batch_size, shuffle=shuffle)\n",
    "\t\t\tdataloader = list(zip(tcycle_dataloader, gcycle_dataloader))\n",
    "\t\t\tfor index, (tbatch, gbatch) in tqdm.tqdm(enumerate(dataloader)):\n",
    "\t\t\t\tg_loss, t_loss = self.back_translation(tbatch, gbatch)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating empty vocabulary object\n",
      "Finished Parsing Text\n"
     ]
    }
   ],
   "source": [
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)\n",
    "\n",
    "vocab = dp.Vocabulary()\n",
    "vocab.parseText(raw_train[0:32])\n",
    "\n",
    "#create cycle\n",
    "\n",
    "cycle_model = CycleModel(vocab)\n",
    "# tcycle_dataloader, gcycle_dataloader = dp.create_cycle_dataloader(vocab.raw_data, batch_size = 8, shuffle=False)\n",
    "\n",
    "# tbatch = tcycle_dataloader[0]\n",
    "# gbatch = gcycle_dataloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating empty vocabulary object\n",
      "Finished Parsing Text\n"
     ]
    }
   ],
   "source": [
    "vocab = dp.Vocabulary()\n",
    "vocab.parseText(raw_train[:34])\n",
    "tcycle_dataloader, gcycle_dataloader = dp.create_cycle_dataloader(vocab.raw_data, batch_size = 8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/salaniz/pycocoevalcap.git\n",
      "  Cloning https://github.com/salaniz/pycocoevalcap.git to /private/var/folders/2v/tg9q212n2_x1mz9848phgdgr0000gn/T/pip-req-build-nca_gkna\n",
      "Collecting pycocotools>=2.0.2\n",
      "  Downloading pycocotools-2.0.3.tar.gz (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 3.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=18.0 in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from pycocotools>=2.0.2->pycocoevalcap==1.2) (50.3.1.post20201107)\n",
      "Collecting cython>=0.27.3\n",
      "  Using cached Cython-0.29.25-py2.py3-none-any.whl (983 kB)\n",
      "Collecting matplotlib>=2.1.0\n",
      "  Downloading matplotlib-3.5.1-cp38-cp38-macosx_10_9_x86_64.whl (7.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.3 MB 16.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.19.4)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.4.0-cp38-cp38-macosx_10_10_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 19.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.2.1 in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (20.7)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp38-cp38-macosx_10_9_x86_64.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 1.0 MB/s \n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.28.3-py3-none-any.whl (884 kB)\n",
      "\u001b[K     |████████████████████████████████| 884 kB 27.8 MB/s \n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap==1.2) (1.15.0)\n",
      "Building wheels for collected packages: pycocoevalcap, pycocotools\n",
      "  Building wheel for pycocoevalcap (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocoevalcap: filename=pycocoevalcap-1.2-py3-none-any.whl size=104312215 sha256=20ff1ced44818f61f0ec7dfeca66de56d2f570b7a2f97911948f4041b2850a81\n",
      "  Stored in directory: /private/var/folders/2v/tg9q212n2_x1mz9848phgdgr0000gn/T/pip-ephem-wheel-cache-e3str669/wheels/bd/05/6e/20e74b95786836ced692ab88d874a1ab5c25f328c519f868e8\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0.3-cp38-cp38-macosx_10_9_x86_64.whl size=89565 sha256=79604243dbe168e86c61fbf0468076d59746b5f3adbf873dcae8d14e798cef1d\n",
      "  Stored in directory: /Users/ramisbahi/Library/Caches/pip/wheels/59/5b/26/04441bc1820bf3622e0ea8616bef01b02cad3415ad880b834a\n",
      "Successfully built pycocoevalcap pycocotools\n",
      "Installing collected packages: cython, pillow, kiwisolver, fonttools, cycler, matplotlib, pycocotools, pycocoevalcap\n",
      "Successfully installed cycler-0.11.0 cython-0.29.25 fonttools-4.28.3 kiwisolver-1.3.2 matplotlib-3.5.1 pillow-8.4.0 pycocoevalcap-1.2 pycocotools-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install \"git+https://github.com/salaniz/pycocoevalcap.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:35, 17.72s/it]\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "cycle_model.train(epochs=1, batch_size=8, learning_rate=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c5ce5b9604fc8445e4e5f27c24807def7cbbefbcd7dbec7e9c2f61ff534743b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
