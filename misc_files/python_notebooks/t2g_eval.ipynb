{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class ModelLSTM(nn.Module):\n",
    "\tdef __init__(self, input_types, relation_types, model_dim, dropout = 0.0):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.word_types = input_types\n",
    "\t\tself.relation_types = relation_types\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.model_dim = model_dim\n",
    "\n",
    "\t\tself.emb = nn.Embedding(input_types, self.model_dim)\n",
    "\t\tself.lstm = nn.LSTM(self.model_dim, self.model_dim//2, batch_first=True, bidirectional=True, num_layers=2)\n",
    "\t\tself.relation_layer1 = nn.Linear(self.model_dim , self.model_dim)\n",
    "\t\tself.relation_layer2 = nn.Linear(self.model_dim , self.model_dim)\n",
    "\t\tself.drop = nn.Dropout(self.dropout)\n",
    "\t\tself.projection = nn.Linear(self.model_dim , self.model_dim)\n",
    "\t\tself.decoder = nn.Linear(self.model_dim , self.relation_types)\n",
    "\t\tself.layer_norm = nn.LayerNorm(self.model_dim)\n",
    "\n",
    "\t\tself.init_params()\n",
    "\n",
    "\tdef init_params(self):\n",
    "\t\tnn.init.xavier_normal_(self.relation_layer1.weight.data)\n",
    "\t\tnn.init.xavier_normal_(self.relation_layer2.weight.data)\n",
    "\t\tnn.init.xavier_normal_(self.projection.weight.data)\n",
    "\t\tnn.init.xavier_normal_(self.decoder.weight.data)\n",
    "\n",
    "\t\tnn.init.constant_(self.relation_layer1.bias.data , 0)\n",
    "\t\tnn.init.constant_(self.relation_layer2.bias.data , 0)\n",
    "\t\tnn.init.constant_(self.projection.bias.data , 0)\n",
    "\t\tnn.init.constant_(self.decoder.bias.data , 0)\n",
    "\n",
    "\t#def forward(self, batch):\n",
    "\tdef forward(self, sents, ent_inds, max_ents):\n",
    "\t\t#sents = batch['text']\n",
    "\t\tsents, (c_0, h_0) = self.lstm(self.emb(sents))\n",
    "\n",
    "\t\tbs, _, hidden_dim = sents.shape\n",
    "\t\t\n",
    "\t\t#max_ents = max([max([ent_ind[0] for ent_ind in batch_ent_inds]) for batch_ent_inds in ent_inds]).item() + 1\n",
    "\t\tmax_ents = max_ents.item()\n",
    "\n",
    "\t\tcont_word_mask = sents.new_zeros(bs, max_ents)\n",
    "\t\tcont_word_embs = sents.new_zeros(bs, max_ents, hidden_dim)\n",
    "\n",
    "\t\t#for b, (sent,entind) in enumerate(zip(sents,batch['entity_inds'])):\n",
    "\t\tfor b, (sent,entind) in enumerate(zip(sents, ent_inds)):\n",
    "\t\t\tfor z in entind:\n",
    "\t\t\t\tif z[0] == -1:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\twordemb = sent[z[1]:z[2]]\n",
    "\t\t\t\t\t#cont_word_embs[b, z[0]] = (cont_word_mask[b, z[0]]*cont_word_embs[b, z[0]] + torch.mean(wordemb, dim = 0))/(cont_word_mask[b, z[0]] + 1)\n",
    "\t\t\t\t\t#FUCK, try ignoring repeats for now :(\n",
    "\t\t\t\t\tcont_word_embs[b, z[0]] = torch.mean(wordemb, dim =0)\n",
    "\t\t\t\t\tcont_word_mask[b, z[0]] = 1\n",
    "\n",
    "\t\t# bs x max_ents x model_dim\n",
    "\t\tcont_word_embs = self.layer_norm(cont_word_embs)\n",
    "\t\tcont_word_mask = torch.clamp(cont_word_mask, 0, 1)\n",
    "\n",
    "\t\trel1 = self.relation_layer1(cont_word_embs)\n",
    "\t\trel2 = self.relation_layer2(cont_word_embs)\n",
    "\n",
    "\t\t#bs x max_ents x max_ents x model_dim\n",
    "\t\tout = rel1.unsqueeze(1) + rel2.unsqueeze(2)\n",
    "\n",
    "\t\tout = F.relu(self.drop(out))\n",
    "\t\tout = F.relu(self.projection(out))\n",
    "\t\tout = self.decoder(out)\n",
    "\n",
    "\t\tout = out * cont_word_mask.view(bs,max_ents,1,1) * cont_word_mask.view(bs,1,max_ents,1)\n",
    "\n",
    "\t\treturn torch.log_softmax(out, -1) # bs x max ents x max_ents x num_relations\n",
    "\n",
    "class T2GModel():\n",
    "\tdef __init__(self, vocab, device, model_dim):\n",
    "\t\tself.inp_types = len(vocab.entities.wordlist) + len(vocab.text.wordlist)\n",
    "\t\tself.rel_types = len(vocab.relations.wordlist)\n",
    "\n",
    "\t\tself.model = ModelLSTM(self.inp_types, self.rel_types, model_dim = model_dim).to(device)\n",
    "\t\tself.vocab = vocab\n",
    "\t\tself.device = device\n",
    "\n",
    "\tdef eval(self):\n",
    "\t\tself.model.eval()\n",
    "    \n",
    "\tdef train(self):\n",
    "\t\tself.model.train()\n",
    "\tdef t2g_preprocess(self, batch):\n",
    "\t\t\t\"\"\" \n",
    "\t\t\t\tinput: list of dictionaries in raw_json_format\n",
    "\t\t\t\toutput: prepreprocessed dictionaries containing text, entity inds\n",
    "\t\t\t\"\"\"\n",
    "\n",
    "\t\t\tdef entity2Indices(entity, mode = \"T2G\"):\n",
    "\t\t\t\ttemp = torch.zeros(len(entity), dtype = torch.long)\n",
    "\t\t\t\tfor ind, word in enumerate(entity):\n",
    "\t\t\t\t\tif word not in self.vocab.entities.word2idx:\n",
    "\t\t\t\t\t\ttemp[ind] = self.vocab.entities.word2idx[\"<UNK>\"]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttemp[ind] = self.vocab.entities.word2idx[word]\n",
    "\t\t\t\treturn temp\n",
    "\t\t\t\t\t\n",
    "\t\t\tdef text2Indices(text):\n",
    "\t\t\t\ttemp = torch.zeros(len(text.split()) + 2, dtype=torch.long)\n",
    "\t\t\t\ttemp[0] = self.vocab.text.word2idx[\"<SOS>\"]\n",
    "\t\t\t\tfor ind, word in enumerate(text.split()):\n",
    "\t\t\t\t\tif word not in self.vocab.text.word2idx:\n",
    "\t\t\t\t\t\ttemp[ind + 1] = self.vocab.text.word2idx[\"<UNK>\"]\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\ttemp[ind + 1] = self.vocab.text.word2idx[word]\n",
    "\t\t\t\ttemp[-1] = self.vocab.text.word2idx[\"<EOS>\"]\n",
    "\t\t\t\treturn temp\n",
    "\n",
    "\t\t\tdef concatTextEntities(raw_json_sentence):\n",
    "\t\t\t\tsent = text2Indices(raw_json_sentence['text'])\n",
    "\t\t\t\tmodified_input = torch.LongTensor([0])\n",
    "\t\t\t\tlbound = 0\n",
    "\t\t\t\tentity_locations = []\n",
    "\t\t\t\tadditional_words = 0\n",
    "\t\t\t\tfor index, value in enumerate(sent):\n",
    "\t\t\t\t\tif value.item() in self.vocab.entityindices:\n",
    "\t\t\t\t\t\ttemp = entity2Indices(raw_json_sentence['entities'][self.vocab.entityindices[value.item()]])\n",
    "\t\t\t\t\t\ttemp += len(self.vocab.text.wordlist)\n",
    "\t\t\t\t\t\tmodified_input = torch.cat((modified_input, sent[lbound:index], temp), dim = 0)\n",
    "\t\t\t\t\t\tentity_locations.append([self.vocab.entityindices[value.item()], index + additional_words, index + additional_words + len(temp)])\n",
    "\t\t\t\t\t\tadditional_words += len(temp) - 1\n",
    "\t\t\t\t\t\tlbound = index + 1\n",
    "\t\t\t\tmodified_input = torch.cat((modified_input, sent[lbound:]), dim = 0)[1:]\n",
    "\t\t\t\treturn modified_input, torch.LongTensor(entity_locations)\n",
    "\n",
    "\t\t\tmaxlentext = 0\n",
    "\t\t\tmaxents = 0\n",
    "\t\t\ttemp_text = []\n",
    "\t\t\ttemp_inds = []\n",
    "\t\t\tfor raw_json_sentence in batch:\n",
    "\t\t\t\t(text, entity_inds) = concatTextEntities(raw_json_sentence)\n",
    "\t\t\t\ttemp_inds.append(entity_inds)\n",
    "\t\t\t\tif len(entity_inds) > maxents:\n",
    "\t\t\t\t\tmaxents = len(entity_inds)\n",
    "\t\t\t\ttemp_text.append(text)\n",
    "\t\t\t\tif text.shape[0] > maxlentext:\n",
    "\t\t\t\t\tmaxlentext = text.shape[0]\n",
    "\t\t\t\t\n",
    "\t\t\tfinal_text = torch.ones((len(batch), maxlentext), dtype = torch.long)*self.vocab.text.word2idx[\"<EMPTY>\"]\n",
    "\t\t\tfinal_ents = torch.ones((len(batch), maxents, 3), dtype = torch.long)*-1\n",
    "\n",
    "\t\t\tfor k in range(len(batch)):\n",
    "\t\t\t\tfinal_text[k][:len(temp_text[k])] = temp_text[k]\n",
    "\t\t\t\tfinal_ents[k][:len(temp_inds[k])] = temp_inds[k]\n",
    "\t\t\t\n",
    "\t\t\treturn final_text, final_ents\n",
    "\n",
    "\t# input - texts with original entities taken out (list of dicts with text and entities)\n",
    "\t# output - batch of graphs (list of dicts with relations and entities)\n",
    "\tdef predict(self, batch):\n",
    "\t\tpreprocessed_text, preprocessed_inds = self.t2g_preprocess(batch)\n",
    "\t\tmax_ents = max([len(graph[\"entities\"]) for graph in batch])\n",
    "\n",
    "\t\t#preds = self.model(preprocessed_text.to(self.device), preprocessed_inds.to(self.device))\n",
    "\t\tpreds = self.model(preprocessed_text.to(self.device), preprocessed_inds.to(self.device), torch.tensor(max_ents).to(self.device))\n",
    "\t\tpreds = torch.argmax(preds, -1)\n",
    "\n",
    "\t\tbs, ne, _ = preds.shape\n",
    "\n",
    "\t\toutput = [] #list of dictionaries\n",
    "\n",
    "\t\t#print(ne, max([len(batch[b]['entities'])] for b in range(bs)))\n",
    "\t\tassert ne == max([len(batch[b]['entities'])] for b in range(bs))[0]\n",
    "\n",
    "\t\tfor b in range(bs):\n",
    "\t\t\ttemp = {\n",
    "\t\t\t\t\"relations\": [],\n",
    "\t\t\t\t\"entities\": batch[b]['entities']\n",
    "\t\t\t}\n",
    "\t\t\tfor i in range(0, ne):\n",
    "\t\t\t\tfor j in range(i+1, ne):\n",
    "\t\t\t\t\ttemp['relations'].append([temp['entities'][i], self.vocab.relations.wordlist[preds[b, i, j]], temp['entities'][j]])\n",
    "\t\t\toutput.append(temp)\n",
    "\t\treturn output\n",
    "\n",
    "\tdef eval_t2g(self, eval_dataset):\n",
    "\t\tdef relation2Indices(raw_json_sentence, max_ents):\n",
    "\t\t\t'''\n",
    "\t\t\t\tParameters:\n",
    "\t\t\t\t\tvocab - Vocabulary object that contains the vocab from a parsed json file\n",
    "\t\t\t\t\traw_json_sentence - one element of array (i.e. one dict) contained in raw json file\n",
    "\t\t\t\t\tmax_ents - gives size of return array\n",
    "\n",
    "\t\t\t\tReturn:\n",
    "\t\t\t\t\tlabels - Symmetrical [max_entities x max_entities)] Longtensor where \n",
    "\t\t\t\t\t\t\t\t\tlabels[i][j] denotes the relation between entities i and j.\n",
    "\t\t\t\t\t\t\tAnything where i >= l or j >= l is <EMPTY> \n",
    "\t\t\t'''\n",
    "\t\t\tl = len(raw_json_sentence['entities'])\n",
    "\t\t\tret = torch.ones((max_ents,max_ents), dtype = torch.long)*self.vocab.relations.word2idx[\"<NO_RELATION>\"]\n",
    "\t\t\tfor i in range(l, max_ents):\n",
    "\t\t\t\tfor j in range(0, max_ents):\n",
    "\t\t\t\t\tret[i][j] = ret[j][i] = self.vocab.relations.word2idx[\"<EMPTY>\"]\n",
    "\t\t\t\n",
    "\t\t\tfor i in range(0, l):\n",
    "\t\t\t\tfor j in range(0, i+1):\n",
    "\t\t\t\t\tret[i][j] = self.vocab.relations.word2idx[\"<EMPTY>\"]\n",
    "\t\t\t\t\t\n",
    "\t\t\t# for i in range(l, max_ents):\n",
    "\t\t\t# \tfor j in range(0, max_ents): # could do (0, l) for efficiency\n",
    "\t\t\t# \t\tret[j][i] = vocab.relations.word2idx[\"<EMPTY>\"]\n",
    "\t\t\tentitydict = {}\n",
    "\t\t\tfor i, entity in enumerate(raw_json_sentence['entities']):\n",
    "\t\t\t\tentitydict[\"\".join(entity)] = i\n",
    "\t\t\tfor relation in raw_json_sentence['relations']:\n",
    "\t\t\t\tind1 = entitydict[\"\".join(relation[0])]\n",
    "\t\t\t\tind2 = entitydict[\"\".join(relation[2])]\n",
    "\t\t\t\t#ret[ind1][ind2] = ret[ind2][ind1] = vocab.relations.word2idx[relation[1]]\n",
    "\t\t\t\tif ind1 < ind2:\n",
    "\t\t\t\t\tret[ind1][ind2] = self.vocab.relations.word2idx[relation[1]]\n",
    "\t\t\t\t\t#ret[ind2][ind1] = self.vocab.relations.word2idx[\"<EMPTY>\"]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tret[ind2][ind1] = self.vocab.relations.word2idx[relation[1]]\n",
    "\t\t\t\t\t#ret[ind1][ind2] = self.vocab.relations.word2idx[\"<EMPTY>\"]\n",
    "\t\t\treturn ret\n",
    "\n",
    "\t\tself.model.eval()\n",
    "\t\tpreprocessed_text, preprocessed_inds = self.t2g_preprocess(eval_dataset)\n",
    "\t\tmax_ents = max([len(graph[\"entities\"]) for graph in eval_dataset])\n",
    "\n",
    "\t\tpreprocessed_labels = [relation2Indices(json_sent, max_ents) for json_sent in eval_dataset]\n",
    "\n",
    "\t\tpreds = self.model(preprocessed_text.to(self.device), preprocessed_inds.to(self.device), torch.tensor(max_ents))\n",
    "\t\tpreds = torch.argmax(preds, -1)\n",
    "\n",
    "\t\tbs, ne, _ = preds.shape\n",
    "\n",
    "\t\ttrue_labels = []\n",
    "\t\tpred_labels = []\n",
    "\n",
    "\t\tfor b in range(bs):\n",
    "\t\t\ttemp_true = []\n",
    "\t\t\ttemp_pred = []\n",
    "\t\t\tfor i in range(0, len(eval_dataset[b]['entities'])):\n",
    "\t\t\t\tfor j in range(i+1, len(eval_dataset[b]['entities'])):\n",
    "\t\t\t\t\ttemp_true.append(preprocessed_labels[b][i][j].item())\n",
    "\t\t\t\t\ttemp_pred.append(preds[b][i][j].item())\n",
    "\t\t\ttrue_labels.extend(temp_true)\n",
    "\t\t\tpred_labels.extend(temp_pred)\n",
    "\n",
    "\t\tprint(\"Micro F1: \", f1_score(true_labels, pred_labels, average = 'micro'))\n",
    "\t\tprint(\"Macro F1: \", f1_score(true_labels, pred_labels, average = 'macro'))\n",
    "\t\t#print(\"true\", true_labels)\n",
    "\t\t#print(\"pred\", pred_labels)\n",
    "\t\treturn f1_score(true_labels, pred_labels, average = 'micro'), f1_score(true_labels, pred_labels, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating empty vocabulary object\n",
      "Finished Parsing Text\n"
     ]
    }
   ],
   "source": [
    "# importing the module\n",
    "import json\n",
    "\n",
    "import data_processing as dp\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)\n",
    "\n",
    "vocab = dp.Vocabulary()\n",
    "vocab.parseText(raw_train)\n",
    "\n",
    "inp_types = len(vocab.entities.wordlist) + len(vocab.text.wordlist)\n",
    "rel_types = len(vocab.relations.wordlist)\n",
    "\n",
    "t2g_model = T2GModel(vocab, torch.device('cpu'), 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('json_datasets/dev.json', 'r')\n",
    "\n",
    "raw_dev = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "def train_model_supervised(model, num_relations, warmup_epochs = 2, learning_rate = 5.0e-5, epochs = 30):\n",
    "\t\"\"\"\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Create model\n",
    "\toptimzer = torch.optim.Adam(model.model.parameters(), lr = learning_rate)\n",
    "\tbest_acc = 0\n",
    "\tstate_dict_clone = {k: v.clone() for k, v in model.model.state_dict().items()}\n",
    "\tlosses = []\n",
    "\tfor p in range(epochs):\n",
    "\t\tmodel.model.train()\n",
    "\t\tif p < warmup_epochs:\n",
    "\t\t\tt, g = dp.create_cycle_dataloader(model.vocab.raw_data[:3550], 32, True)\n",
    "\t\t\tdataloader = t+g\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tt, g = dp.create_cycle_dataloader(model.vocab.raw_data, 32, True)\n",
    "\t\t\tdataloader = t+g\n",
    "\t\t\n",
    "\t\tloss_this_epoch = 0.0\n",
    "\t\tfor index, batch in tqdm.tqdm(enumerate(dataloader)):\n",
    "\t\t\tpre_text, pre_ents = model.t2g_preprocess(batch)\n",
    "\n",
    "\t\t\tbs, _ = pre_text.shape\n",
    "\n",
    "\t\t\tmax_ents = max([len(ex['entities']) for ex in batch])\n",
    "\n",
    "\t\t\tlabels = torch.zeros((bs, max_ents, max_ents), dtype = torch.long)\n",
    "\t\t\tfor k, raw_json in enumerate(batch):\n",
    "\t\t\t\tlabels[k] = dp.relation2Indices(vocab, raw_json, max_ents)\n",
    "    \n",
    "\t\t\tlog_probs = model.model(pre_text, pre_ents, torch.tensor(max_ents))\n",
    "\n",
    "\t\t\tloss = F.nll_loss(log_probs.view(-1, num_relations), labels.view(-1), ignore_index = 0)\n",
    "\t\t\tloss_this_epoch += loss.item()\n",
    "\t\t\toptimzer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\t# torch.nn.utils.clip_grad_norm_(\n",
    "\t\t\t#     [p for group in optimzer.param_groups for p in group['params']], CLIP)\n",
    "\t\t\toptimzer.step()\n",
    "\n",
    "\t\tprint(\"Evaluating: \")\n",
    "\t\tmicro, macro = model.eval_t2g(raw_dev)\n",
    "\t\tif (micro+macro)/2.0 > best_acc:\n",
    "\t\t\tbest_acc = (micro+macro)/2.0\n",
    "\t\t\tcurr_state_dict = model.model.state_dict()\n",
    "\t\t\tfor key in state_dict_clone.keys():\n",
    "\t\t\t\tstate_dict_clone[key].copy_(curr_state_dict[key])\n",
    "\t\t\n",
    "\n",
    "\t\tprint(\"Loss this epoch: \", loss_this_epoch/len(dataloader)*32)\n",
    "\t\tlosses.append(loss_this_epoch/len(dataloader)*32)\n",
    "\n",
    "\tcurr_state_dict = model.model.state_dict()\n",
    "\tfor key in state_dict_clone.keys():\n",
    "\t\tcurr_state_dict[key].copy_(state_dict_clone[key])\n",
    "\n",
    "\tprint(\"saving model\")\n",
    "\ttorch.save(model.model.state_dict(), \"./sup_t2g_dict\")\n",
    "\treturn curr_state_dict, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:41,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.10256985747951969\n",
      "Macro F1:  0.020639259881042257\n",
      "Loss this epoch:  134.56762777056014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:40,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.15688474918639883\n",
      "Macro F1:  0.08757101096737306\n",
      "Loss this epoch:  75.90961483546666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [05:45,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.7984513522612502\n",
      "Macro F1:  0.41041169228494956\n",
      "Loss this epoch:  46.52543717272141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [05:43,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.8950735046571653\n",
      "Macro F1:  0.6222581897242923\n",
      "Loss this epoch:  15.812455640119666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:27,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9316575019638649\n",
      "Macro F1:  0.7369265499257677\n",
      "Loss this epoch:  7.417939630209231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:07,  1.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9516328133767255\n",
      "Macro F1:  0.7884274086130676\n",
      "Loss this epoch:  4.151679807726075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:04,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9596004937717428\n",
      "Macro F1:  0.8426645644101688\n",
      "Loss this epoch:  2.563527770924802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:04,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9622937941869599\n",
      "Macro F1:  0.8544705550449725\n",
      "Loss this epoch:  1.7342303490229682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:01,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9654359780047133\n",
      "Macro F1:  0.8646852309431722\n",
      "Loss this epoch:  1.2827048943761516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:02,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9668948490629559\n",
      "Macro F1:  0.8761034588833174\n",
      "Loss this epoch:  1.0194586652020614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:04,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9688026035237347\n",
      "Macro F1:  0.8946804799622127\n",
      "Loss this epoch:  0.7947662187849774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:03,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9720570081921222\n",
      "Macro F1:  0.8961225801227383\n",
      "Loss this epoch:  0.6850046170327593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:05,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9673437324654921\n",
      "Macro F1:  0.8954791695695714\n",
      "Loss this epoch:  0.6628498239807931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:01,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9690270452250028\n",
      "Macro F1:  0.8876145379617635\n",
      "Loss this epoch:  0.5700151898952968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:03,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9699248120300752\n",
      "Macro F1:  0.8836746189384511\n",
      "Loss this epoch:  0.4636322893202305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:05,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.971832566490854\n",
      "Macro F1:  0.9036148158809617\n",
      "Loss this epoch:  0.442430134068298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:04,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.969475928627539\n",
      "Macro F1:  0.8876550256672806\n",
      "Loss this epoch:  0.45509810025310693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:01,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.971832566490854\n",
      "Macro F1:  0.9026897315275367\n",
      "Loss this epoch:  0.41293525420056254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:02,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9730669958478285\n",
      "Macro F1:  0.8999757413975797\n",
      "Loss this epoch:  0.3720997248949739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:04,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9729547749971945\n",
      "Macro F1:  0.9019850823194979\n",
      "Loss this epoch:  0.31292752357309356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:05,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9716081247895859\n",
      "Macro F1:  0.8924938395076712\n",
      "Loss this epoch:  0.27733973386770516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:04,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9700370328807092\n",
      "Macro F1:  0.8988398660329184\n",
      "Loss this epoch:  0.2934721491947843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:03,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9714959039389519\n",
      "Macro F1:  0.8974078175441407\n",
      "Loss this epoch:  0.289878543160454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:03,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9736281001009988\n",
      "Macro F1:  0.9031680059632867\n",
      "Loss this epoch:  0.23992784538616738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:03,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9729547749971945\n",
      "Macro F1:  0.9057935128850051\n",
      "Loss this epoch:  0.26739929475383284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:03,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9736281001009988\n",
      "Macro F1:  0.8974741168414622\n",
      "Loss this epoch:  0.2598085780536719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:02,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9753114128605095\n",
      "Macro F1:  0.8966901390815396\n",
      "Loss this epoch:  0.21064299520095994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:03,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.974301425204803\n",
      "Macro F1:  0.9030875026831483\n",
      "Loss this epoch:  0.18124522343871421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:04,  1.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9739647626529009\n",
      "Macro F1:  0.9080017229086347\n",
      "Loss this epoch:  0.1865051279652535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "408it [04:05,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: \n",
      "Micro F1:  0.9730669958478285\n",
      "Macro F1:  0.8978776372059504\n",
      "Loss this epoch:  0.22467787036964415\n",
      "saving model\n"
     ]
    }
   ],
   "source": [
    "state_dict, losses = train_model_supervised(t2g_model, rel_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newt2g = T2GModel(vocab, torch.device('cpu'), 512)\n",
    "newt2g.model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5555555555555555"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [torch.tensor(1),torch.tensor(0),torch.tensor(1),torch.tensor(0)]\n",
    "y_pred = [torch.tensor(2),torch.tensor(0),torch.tensor(1),torch.tensor(0)]\n",
    "\n",
    "f1_score(y_true=y_true, y_pred=y_pred, average = \"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c5ce5b9604fc8445e4e5f27c24807def7cbbefbcd7dbec7e9c2f61ff534743b"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
