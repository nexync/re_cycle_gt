{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38564bitbaseconda53d3aa08a74d40a8860ef501a21f2398",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Collecting stanza\n  Downloading stanza-1.3.0-py3-none-any.whl (432 kB)\n\u001b[K     |████████████████████████████████| 432 kB 1.6 MB/s \n\u001b[?25hRequirement already satisfied: requests in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from stanza) (2.24.0)\nRequirement already satisfied: tqdm in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from stanza) (4.51.0)\nRequirement already satisfied: torch>=1.3.0 in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from stanza) (1.7.0)\nRequirement already satisfied: six in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from stanza) (1.15.0)\nRequirement already satisfied: protobuf in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from stanza) (3.19.0)\nRequirement already satisfied: numpy in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from stanza) (1.19.4)\nCollecting emoji\n  Downloading emoji-1.6.1.tar.gz (170 kB)\n\u001b[K     |████████████████████████████████| 170 kB 2.3 MB/s \n\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from requests->stanza) (1.25.11)\nRequirement already satisfied: certifi>=2017.4.17 in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from requests->stanza) (2020.6.20)\nRequirement already satisfied: chardet<4,>=3.0.2 in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from requests->stanza) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from requests->stanza) (2.10)\nRequirement already satisfied: future in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from torch>=1.3.0->stanza) (0.18.2)\nRequirement already satisfied: dataclasses in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from torch>=1.3.0->stanza) (0.6)\nRequirement already satisfied: typing-extensions in /Users/ramisbahi/opt/miniconda3/lib/python3.8/site-packages (from torch>=1.3.0->stanza) (3.7.4.3)\nBuilding wheels for collected packages: emoji\n  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for emoji: filename=emoji-1.6.1-py3-none-any.whl size=169295 sha256=e293e42e4d4bc445c5c0174263f2b6446491663523c7de21f6b6f0bd268a1b1c\n  Stored in directory: /Users/ramisbahi/Library/Caches/pip/wheels/04/29/50/1e7189f03d2cf139e469863d54a1d3eabeb10c92c84e51f8a1\nSuccessfully built emoji\nInstalling collected packages: emoji, stanza\nSuccessfully installed emoji-1.6.1 stanza-1.3.0\nNote: you may need to restart the kernel to use updated packages.\n"
    }
   ],
   "source": [
    "pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json: 142kB [00:00, 17.5MB/s]\n2021-11-27 21:47:06 INFO: Downloading default packages for language: en (English)...\n2021-11-27 21:47:08 INFO: File exists: /Users/ramisbahi/stanza_resources/en/default.zip.\n2021-11-27 21:47:12 INFO: Finished downloading models and saved to /Users/ramisbahi/stanza_resources.\n"
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "\n",
    "stanza.download('en') # download English model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2021-11-27 21:46:57 INFO: Loading these models for language: en (English):\n============================\n| Processor    | Package   |\n----------------------------\n| tokenize     | combined  |\n| pos          | combined  |\n| lemma        | combined  |\n| depparse     | combined  |\n| sentiment    | sstplus   |\n| constituency | wsj       |\n| ner          | ontonotes |\n============================\n\n2021-11-27 21:46:57 INFO: Use device: cpu\n2021-11-27 21:46:57 INFO: Loading: tokenize\n2021-11-27 21:46:57 INFO: Loading: pos\n2021-11-27 21:46:57 INFO: Loading: lemma\n2021-11-27 21:46:57 INFO: Loading: depparse\n2021-11-27 21:46:58 INFO: Loading: sentiment\n2021-11-27 21:46:58 INFO: Loading: constituency\n2021-11-27 21:46:58 INFO: Loading: ner\n2021-11-27 21:46:59 INFO: Done loading processors!\n"
    }
   ],
   "source": [
    "ent_extractor = stanza.Pipeline('en') # initialize English neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Arhes Airport is 5.000 meters long .\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'text': '<ENT_0> is <ENT_1> long .',\n 'entities': [['Arhes', 'Airport'], ['5.000', 'meters']]}"
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "def padded_text(text):\n",
    "    out_text = re.sub( r'([a-zA-Z])([,.!])', r'\\1 \\2', text)\n",
    "    out_text = re.sub('\\s{2,}', ' ', out_text) # collapses multiple spaces\n",
    "    return out_text.lstrip().rstrip()\n",
    "\n",
    "def get_text_entities(text):\n",
    "    doc = ent_extractor(text).entities\n",
    "    ents = [entity.text for entity in doc]\n",
    "    print(text)\n",
    "    out_text = text\n",
    "    for i in range(len(ents)):\n",
    "        out_text = out_text.replace(ents[i], \"<ENT_\" + str(i) + \">\")\n",
    "\n",
    "    out_ents = [[word for word in ent.split()] for ent in ents]\n",
    "    \n",
    "    return {'text' : out_text, 'entities' : out_ents}\n",
    "\n",
    "in_text = \"Arhes Airport is 5.000 meters long.\"\n",
    "text = padded_text(in_text)\n",
    "get_text_entities(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}