{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the module\n",
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabCategory():\n",
    "\tdef __init__(self):\n",
    "\t\tself.wordlist = []\n",
    "\t\tself.word2idx = {}\n",
    "\t\tself.wordfreq = Counter()\n",
    "\n",
    "class Vocabulary():\n",
    "\tdef __init__(self):\n",
    "\t\tprint(\"Creating empty vocabulary object\")\n",
    "\t\tself.text = VocabCategory()\n",
    "\t\tself.entities = VocabCategory()\n",
    "\t\tself.relations = VocabCategory()\n",
    "\n",
    "\t\tself.relations.word2idx[\"<NO_RELATION>\"] = len(self.relations.wordlist) # no relation token for relations vocab\n",
    "\t\tself.relations.wordlist.append(\"<NO_RELATION>\")\n",
    "\n",
    "\t\tself.init_vocab(self.text)\n",
    "\t\tself.init_vocab(self.entities)\n",
    "\t\tself.init_vocab(self.relations)\n",
    "\t\t\n",
    "\t# initializes UNK, SOS, EOS, and EMPTY tokens\n",
    "\tdef init_vocab(self, vocab_category):\n",
    "\t\tvocab_category.word2idx[\"<UNK>\"] = len(vocab_category.wordlist)\n",
    "\t\tvocab_category.wordlist.append(\"<UNK>\")\n",
    "\t\tvocab_category.word2idx[\"<SOS>\"] = len(vocab_category.wordlist)\n",
    "\t\tvocab_category.wordlist.append(\"<SOS>\")\n",
    "\t\tvocab_category.word2idx[\"<EOS>\"] = len(vocab_category.wordlist)\n",
    "\t\tvocab_category.wordlist.append(\"<EOS>\")\n",
    "\t\tvocab_category.word2idx[\"<EMPTY>\"] = len(vocab_category.wordlist)\n",
    "\t\tvocab_category.wordlist.append(\"<EMPTY>\")\n",
    "\t\t\n",
    "\n",
    "\tdef parseSentence(self, raw_json_sentence):\n",
    "\t\tfor relation in raw_json_sentence['relations']: #Relation parsing here\n",
    "\t\t\tassert len(relation) == 3, \"CHECK THIS!\"\n",
    "\t\t\tif relation[1] not in self.relations.word2idx:\n",
    "\t\t\t\tself.relations.word2idx[relation[1]] = len(self.relations.wordlist)\n",
    "\t\t\t\tself.relations.wordlist.append(relation[1])\n",
    "\t\t\tself.relations.wordfreq.update({relation[1]: 1})\n",
    "\t\t\n",
    "\t\tfor word in raw_json_sentence['text'].split(): #Word parsing here\n",
    "\t\t\tif word not in self.text.word2idx:\n",
    "\t\t\t\tself.text.word2idx[word] = len(self.text.wordlist)\n",
    "\t\t\t\tself.text.wordlist.append(word)\n",
    "\t\tself.text.wordfreq += Counter(raw_json_sentence['text'].split())\n",
    "\n",
    "\t\tfor entity in raw_json_sentence['entities']:\n",
    "\t\t\tfor e in entity:\n",
    "\t\t\t\tif e not in self.entities.word2idx:\n",
    "\t\t\t\t\tself.entities.word2idx[e] = len(self.entities.wordlist)\n",
    "\t\t\t\t\tself.entities.wordlist.append(e)\n",
    "\t\t\tself.entities.wordfreq += Counter(entity)\n",
    "\t\n",
    "\tdef parseText(self, raw_json):\n",
    "\t\tfor raw_sentence in raw_json:\n",
    "\t\t\tself.parseSentence(raw_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating empty vocabulary object\n"
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.parseText(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'relations': [[['Abilene', 'Regional', 'Airport'],\n   'runwayLength',\n   ['2195.0']]],\n 'text': 'the runway length of <ENT_0> is <ENT_1> .',\n 'entities': [['Abilene', 'Regional', 'Airport'], ['2195.0']]}"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "raw_train[54]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'AbileneRegionalAirport': 0, '2195.0': 1}\n[['Abilene', 'Regional', 'Airport'], 'runwayLength', ['2195.0']]\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[0, 9],\n        [9, 0]])"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "# outputs ret (2d tensor) showing relations between entities i, j at ret[i][j]\n",
    "def text2Relation(vocab, raw_json_sentence):\n",
    "\tl = len(raw_json_sentence['entities'])\n",
    "\tret = torch.zeros((l,l), dtype = torch.long)\n",
    "\tentitydict = {}\n",
    "\tfor i, entity in enumerate(raw_json_sentence['entities']):\n",
    "\t\tentitydict[\"\".join(entity)] = i\n",
    "\tprint(entitydict)\n",
    "\tfor relation in raw_json_sentence['relations']:\n",
    "\t\tprint(relation)\n",
    "\t\tind1 = entitydict[\"\".join(relation[0])]\n",
    "\t\tind2 = entitydict[\"\".join(relation[2])]\n",
    "\t\tret[ind1][ind2] = ret[ind2][ind1] = vocab.relations.word2idx[relation[1]]\n",
    "\treturn ret\n",
    "\n",
    "#print(vocab.relations.word2idx[\"<NO_RELATION>\"])\n",
    "text2Relation(vocab, raw_train[54])\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: need SOS/EOS for entity2indices?\n",
    "def entity2Indices(vocab, entity):\n",
    "\ttemp = torch.zeros(len(entity), dtype = torch.long)\n",
    "\tfor ind, word in enumerate(entity):\n",
    "\t\tif word not in vocab.entities.word2idx:\n",
    "\t\t\ttemp[ind] = vocab.entities.word2idx[\"<UNK>\"]\n",
    "\t\telse:\n",
    "\t\t\ttemp[ind] = vocab.entities.word2idx[word]\n",
    "\treturn temp\n",
    "\t\t\n",
    "def text2Indices(vocab, text):\n",
    "\ttemp = torch.zeros(len(text.split()) + 2, dtype=torch.long)\n",
    "\ttemp[0] = vocab.text.word2idx[\"<SOS>\"]\n",
    "\tfor ind, word in enumerate(text.split()):\n",
    "\t\tif word not in vocab.text.word2idx:\n",
    "\t\t\ttemp[ind + 1] = vocab.text.word2idx[\"<UNK>\"]\n",
    "\t\telse:\n",
    "\t\t\ttemp[ind + 1] = vocab.text.word2idx[word]\n",
    "\ttemp[len(text.split()) + 1] = vocab.text.word2idx[\"<EOS>\"]\n",
    "\treturn temp\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'relations': [[['Abilene', ',', 'Texas'],\n   'isPartOf',\n   ['Jones', 'County', ',', 'Texas']]],\n 'text': '<ENT_0> is part of <ENT_1> .',\n 'entities': [['Abilene', ',', 'Texas'], ['Jones', 'County', ',', 'Texas']]}"
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "raw_train[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatTextEntities(vocab, raw_json_sentence, entity_indices):\n",
    "\tsent = text2Indices(vocab, raw_json_sentence['text'])\n",
    "\tmodified_input = torch.LongTensor([0])\n",
    "\tlbound = 0\n",
    "\tentity_locations = []\n",
    "\tadditional_words = 0\n",
    "\tfor index, value in enumerate(sent):\n",
    "\t\tif value.item() in entity_indices:\n",
    "\t\t\ttemp = entity2Indices(vocab, raw_json_sentence['entities'][entity_indices[value.item()]])\n",
    "\t\t\ttemp += len(vocab.text.wordlist)\n",
    "\t\t\tmodified_input = torch.cat((modified_input, sent[lbound:index], temp), dim = 0)\n",
    "\t\t\tentity_locations.append((index + additional_words, index + additional_words + len(temp)))\n",
    "\t\t\tadditional_words += len(temp) - 1\n",
    "\t\t\tlbound = index + 1\n",
    "\tmodified_input = torch.cat((modified_input, sent[lbound:]), dim = 0)[1:]\n",
    "\n",
    "\treturn modified_input, entity_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{3: 0, 1: 1, 721: 2, 959: 3, 1131: 4, 1343: 5, 1445: 6, 1471: 7}\n['<ENT_0>', '<ENT_1>', '<ENT_2>', '<ENT_3>', '<ENT_4>', '<ENT_5>', '<ENT_6>', '<ENT_7>']\n"
    }
   ],
   "source": [
    "entity_indices = {}\n",
    "i = 0\n",
    "while True:\n",
    "\tif '<ENT_' + str(i) + '>' in vocab.text.word2idx:\n",
    "\t\tentity_indices[(vocab.text.word2idx['<ENT_' + str(i) + '>'])] = i\n",
    "\t\ti += 1\n",
    "\telse:\n",
    "\t\tbreak\n",
    "\n",
    "print(entity_indices)\n",
    "print([vocab.text.wordlist[k] for k in entity_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e04db361a4f8367a2d031f37306cd7dd06d0ce8a25d39dd9e4b8b2f9ddb79174"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python38564bitbaseconda53d3aa08a74d40a8860ef501a21f2398"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}