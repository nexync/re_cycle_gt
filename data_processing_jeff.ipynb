{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import random\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the module\n",
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)\n",
    "\n",
    "f = open('json_datasets/test.json', 'r')\n",
    "raw_test = json.load(f)\n",
    "\n",
    "f = open('json_datasets/dev.json', 'r')\n",
    "raw_dev = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relations': [[['Aarhus', 'Airport'],\n",
       "   'cityServed',\n",
       "   ['``', 'Aarhus', ',', 'Denmark', \"''\"]]],\n",
       " 'text': 'the <ENT_1> of <ENT_0> .',\n",
       " 'entities': [['``', 'Aarhus', ',', 'Denmark', \"''\"], ['Aarhus', 'Airport']]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabCategory():\n",
    "\tdef __init__(self):\n",
    "\t\tself.wordlist = []\n",
    "\t\tself.word2idx = {}\n",
    "\t\tself.wordfreq = Counter()\n",
    "\n",
    "class Vocabulary():\n",
    "\tdef __init__(self):\n",
    "\t\tprint(\"Creating empty vocabulary object\")\n",
    "\t\tself.text = VocabCategory()\n",
    "\t\tself.entities = VocabCategory()\n",
    "\t\tself.relations = VocabCategory()\n",
    "\n",
    "\t\tself.init_vocab(self.text)\n",
    "\t\tself.init_vocab(self.entities)\n",
    "\t\tself.init_vocab(self.relations)\n",
    "\t\t\n",
    "\t\tself.relations.word2idx[\"<NO_RELATION>\"] = len(self.relations.wordlist) # no relation token for relations vocab\n",
    "\t\tself.relations.wordlist.append(\"<NO_RELATION>\")\n",
    "\t\t\n",
    "\t# initializes UNK, SOS, EOS, and EMPTY tokens\n",
    "\tdef init_vocab(self, vocab_category):\n",
    "\t\ttokens = [\"<EMPTY>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]\n",
    "\n",
    "\t\tfor token in tokens:\n",
    "\t\t\tvocab_category.word2idx[token] = len(vocab_category.wordlist)\n",
    "\t\t\tvocab_category.wordlist.append(token)\n",
    "\t\t# vocab_category.word2idx[\"<UNK>\"] = len(vocab_category.wordlist)\n",
    "\t\t# vocab_category.wordlist.append(\"<UNK>\")\n",
    "\t\t# vocab_category.word2idx[\"<SOS>\"] = len(vocab_category.wordlist)\n",
    "\t\t# vocab_category.wordlist.append(\"<SOS>\")\n",
    "\t\t# vocab_category.word2idx[\"<EOS>\"] = len(vocab_category.wordlist)\n",
    "\t\t# vocab_category.wordlist.append(\"<EOS>\")\n",
    "\t\t# vocab_category.word2idx[\"<EMPTY>\"] = len(vocab_category.wordlist)\n",
    "\t\t# vocab_category.wordlist.append(\"<EMPTY>\")\n",
    "\t\t\n",
    "\n",
    "\tdef parseSentence(self, raw_json_sentence):\n",
    "\t\tfor relation in raw_json_sentence['relations']: #Relation parsing here\n",
    "\t\t\tassert len(relation) == 3, \"CHECK THIS!\"\n",
    "\t\t\tif relation[1] not in self.relations.word2idx:\n",
    "\t\t\t\tself.relations.word2idx[relation[1]] = len(self.relations.wordlist)\n",
    "\t\t\t\tself.relations.wordlist.append(relation[1])\n",
    "\t\t\tself.relations.wordfreq.update({relation[1]: 1})\n",
    "\t\t\n",
    "\t\tfor word in raw_json_sentence['text'].split(): #Word parsing here\n",
    "\t\t\tif word not in self.text.word2idx:\n",
    "\t\t\t\tself.text.word2idx[word] = len(self.text.wordlist)\n",
    "\t\t\t\tself.text.wordlist.append(word)\n",
    "\t\tself.text.wordfreq += Counter(raw_json_sentence['text'].split())\n",
    "\n",
    "\t\tfor entity in raw_json_sentence['entities']:\n",
    "\t\t\tfor e in entity:\n",
    "\t\t\t\tif e not in self.entities.word2idx:\n",
    "\t\t\t\t\tself.entities.word2idx[e] = len(self.entities.wordlist)\n",
    "\t\t\t\t\tself.entities.wordlist.append(e)\n",
    "\t\t\tself.entities.wordfreq += Counter(entity)\n",
    "\t\n",
    "\tdef parseText(self, raw_json):\n",
    "\t\tfor raw_sentence in raw_json:\n",
    "\t\t\tself.parseSentence(raw_sentence)\n",
    "\t\tprint(\"Finished Parsing Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating empty vocabulary object\n",
      "Finished Parsing Text\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.parseText(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relations': [[['Abilene', 'Regional', 'Airport'], 'runwayLength', ['2195.0']]], 'text': 'the runway length of <ENT_0> is <ENT_1> .', 'entities': [['Abilene', 'Regional', 'Airport'], ['2195.0']]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4, 9],\n",
       "        [9, 4]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def entity2Indices(vocab, entity):\n",
    "\ttemp = torch.zeros(len(entity), dtype = torch.long)\n",
    "\tfor ind, word in enumerate(entity):\n",
    "\t\tif word not in vocab.entities.word2idx:\n",
    "\t\t\ttemp[ind] = vocab.entities.word2idx[\"<UNK>\"]\n",
    "\t\telse:\n",
    "\t\t\ttemp[ind] = vocab.entities.word2idx[word]\n",
    "\treturn temp\n",
    "\t\t\n",
    "def text2Indices(vocab, text):\n",
    "\ttemp = torch.zeros(len(text.split()) + 2, dtype=torch.long)\n",
    "\ttemp[0] = vocab.text.word2idx[\"<SOS>\"]\n",
    "\tfor ind, word in enumerate(text.split()):\n",
    "\t\tif word not in vocab.text.word2idx:\n",
    "\t\t\ttemp[ind + 1] = vocab.text.word2idx[\"<UNK>\"]\n",
    "\t\telse:\n",
    "\t\t\ttemp[ind + 1] = vocab.text.word2idx[word]\n",
    "\ttemp[-1] = vocab.text.word2idx[\"<EOS>\"]\n",
    "\treturn temp\n",
    "\n",
    "def relation2Indices(vocab, raw_json_sentence):\n",
    "\tl = len(raw_json_sentence['entities'])\n",
    "\tret = torch.ones((l,l), dtype = torch.long)*vocab.relations.word2idx[\"<NO_RELATION>\"]\n",
    "\tentitydict = {}\n",
    "\tfor i, entity in enumerate(raw_json_sentence['entities']):\n",
    "\t\tentitydict[\"\".join(entity)] = i\n",
    "\tfor relation in raw_json_sentence['relations']:\n",
    "\t\tind1 = entitydict[\"\".join(relation[0])]\n",
    "\t\tind2 = entitydict[\"\".join(relation[2])]\n",
    "\t\tret[ind1][ind2] = ret[ind2][ind1] = vocab.relations.word2idx[relation[1]]\n",
    "\treturn ret\n",
    "\n",
    "print(raw_train[54])\n",
    "relation2Indices(vocab, raw_train[54])\n",
    "\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatTextEntities(vocab, raw_json_sentence, entity_indices):\n",
    "\tsent = text2Indices(vocab, raw_json_sentence['text'])\n",
    "\tmodified_input = torch.LongTensor([0])\n",
    "\tlbound = 0\n",
    "\tentity_locations = []\n",
    "\tadditional_words = 0\n",
    "\tfor index, value in enumerate(sent):\n",
    "\t\tif value.item() in entity_indices:\n",
    "\t\t\ttemp = entity2Indices(vocab, raw_json_sentence['entities'][entity_indices[value.item()]])\n",
    "\t\t\ttemp += len(vocab.text.wordlist)\n",
    "\t\t\tmodified_input = torch.cat((modified_input, sent[lbound:index], temp), dim = 0)\n",
    "\t\t\tentity_locations.append([index + additional_words, index + additional_words + len(temp)])\n",
    "\t\t\tadditional_words += len(temp) - 1\n",
    "\t\t\tlbound = index + 1\n",
    "\tmodified_input = torch.cat((modified_input, sent[lbound:]), dim = 0)[1:]\n",
    "\treturn modified_input, torch.tensor(entity_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   2,    4, 1493, 1497,    6, 1492, 1493, 1494, 1495, 1496,    8,    3]),\n",
       " tensor([[ 2,  4],\n",
       "         [ 5, 10]]))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatTextEntities(vocab, raw_train[0], entity_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{7: 0, 5: 1, 725: 2, 963: 3, 1135: 4, 1347: 5, 1449: 6, 1475: 7}\n",
      "['<ENT_0>', '<ENT_1>', '<ENT_2>', '<ENT_3>', '<ENT_4>', '<ENT_5>', '<ENT_6>', '<ENT_7>']\n"
     ]
    }
   ],
   "source": [
    "def getEntityIndices(vocab):\n",
    "\tentity_indices = {}\n",
    "\ti = 0\n",
    "\twhile True:\n",
    "\t\tif '<ENT_' + str(i) + '>' in vocab.text.word2idx:\n",
    "\t\t\tentity_indices[(vocab.text.word2idx['<ENT_' + str(i) + '>'])] = i\n",
    "\t\t\ti += 1\n",
    "\t\telse:\n",
    "\t\t\treturn entity_indices\n",
    "\n",
    "\n",
    "entity_indices = getEntityIndices(vocab)\n",
    "print(entity_indices)\n",
    "print([vocab.text.wordlist[k] for k in entity_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class text2GraphDataset(Dataset):\n",
    "\tdef __init__(self, raw_json_file):\n",
    "\t\tprint(\"Creating custom dataset for T2G task\")\n",
    "\t\t\n",
    "\t\tself.vocab = Vocabulary()\n",
    "\t\tself.vocab.parseText(raw_json_file)\n",
    "\t\t\n",
    "\t\tself.inputs = []\n",
    "\t\tself.labels = []\n",
    "\t\t\n",
    "\t\tself.entity_indices = getEntityIndices(self.vocab)\n",
    "\n",
    "\t\tfor raw_json_sentence in raw_json_file:\n",
    "\t\t\tself.labels.append(relation2Indices(self.vocab, raw_json_sentence))\n",
    "\t\t\tself.inputs.append(concatTextEntities(self.vocab, raw_json_sentence, self.entity_indices))\n",
    "\n",
    "\t\tprint(\"Finished processing raw json file\")\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.inputs)\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.inputs[idx], self.labels[idx]\n",
    "\n",
    "def getBatches(vocab, dataset, batch_size, shuffle = False):\n",
    "\tdef create_dict(dataset, indices):\n",
    "\t\ttempdict = {\n",
    "\t\t\t\"entity_inds\": [],\n",
    "\t\t\t\"text_lengths\": [],\n",
    "\t\t\t\"entity_lengths\": []\n",
    "\t\t}\n",
    "\t\t\n",
    "\t\tfor index in indices:\n",
    "\t\t\t(text, entity), label = dataset[index]\n",
    "\t\t\ttempdict[\"entity_inds\"].append(entity)\n",
    "\t\t\ttempdict[\"text_lengths\"].append(text.shape[0])\n",
    "\t\t\ttempdict[\"entity_lengths\"].append(label.shape[0])\n",
    "\t\t\t#tempdict[\"text\"].append(text)\n",
    "\t\t\t#tempdict[\"labels\"].append(label)\t\n",
    "\t\tmaxlentext = max(tempdict[\"text_lengths\"])\n",
    "\t\tmaxlenentity = max(tempdict[\"entity_lengths\"])\n",
    "\n",
    "\t\tfinal_text = torch.ones((len(indices), maxlentext), dtype = torch.long)*vocab.text.word2idx[\"<EMPTY>\"]\n",
    "\t\tfinal_label = torch.ones((len(indices), maxlenentity, maxlenentity), dtype = torch.long)*vocab.relations.word2idx[\"<EMPTY>\"]\n",
    "\t\tfor k, index in enumerate(indices):\n",
    "\t\t\t(text, entity), label = dataset[index]\n",
    "\t\t\tfinal_text[k][:text.shape[0]] = text\n",
    "\t\t\tfinal_label[k][:label.shape[0],:label.shape[1]] = label\n",
    "\n",
    "\t\ttempdict[\"text\"] = final_text\n",
    "\t\ttempdict[\"labels\"] = final_label\n",
    "\t\treturn tempdict\n",
    "\n",
    "\tindices = np.arange(0, len(dataset))\n",
    "\tif shuffle:\n",
    "\t\trandom.shuffle(indices)\n",
    "\t\n",
    "\tassert len(indices) == len(dataset), \"Check length\"\n",
    "\tbatches = []\n",
    "\tcurrIndex = 0\n",
    "\twhile currIndex + batch_size <= len(dataset):\n",
    "\t\ttempdict = create_dict(dataset, indices[currIndex: currIndex + batch_size])\n",
    "\t\tbatches.append(tempdict)\n",
    "\t\tcurrIndex += batch_size\n",
    "\n",
    "\tif currIndex < len(dataset):\n",
    "\t\ttempdict = create_dict(dataset, indices[currIndex:])\n",
    "\t\tbatches.append(tempdict)\n",
    "\treturn batches\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating custom dataset for T2G task\n",
      "Creating empty vocabulary object\n",
      "Finished Parsing Text\n",
      "Finished processing raw json file\n"
     ]
    }
   ],
   "source": [
    "dataset = text2GraphDataset(raw_json_file = raw_train)\n",
    "dataloader = getBatches(vocab, dataset, batch_size = 8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entity_inds': [tensor([[ 2,  4],\n",
       "          [ 5, 10]]),\n",
       "  tensor([[ 1,  3],\n",
       "          [ 7, 12]]),\n",
       "  tensor([[1, 3],\n",
       "          [7, 8]]),\n",
       "  tensor([[1, 3],\n",
       "          [4, 5]]),\n",
       "  tensor([[1, 3],\n",
       "          [8, 9]]),\n",
       "  tensor([[1, 3],\n",
       "          [4, 5]]),\n",
       "  tensor([[1, 3],\n",
       "          [6, 7]]),\n",
       "  tensor([[4, 6],\n",
       "          [7, 8]])],\n",
       " 'text_lengths': [12, 14, 10, 11, 15, 12, 9, 10],\n",
       " 'entity_lengths': [2, 2, 2, 2, 2, 2, 2, 2],\n",
       " 'text': tensor([[   2,    4, 1493, 1497,    6, 1492, 1493, 1494, 1495, 1496,    8,    3,\n",
       "             0,    0,    0],\n",
       "         [   2, 1493, 1497,    9,    4,   10,    6, 1492, 1493, 1494, 1495, 1496,\n",
       "             8,    3,    0],\n",
       "         [   2, 1493, 1497,    9,    4,   10,    6, 1493,    8,    3,    0,    0,\n",
       "             0,    0,    0],\n",
       "         [   2, 1493, 1497,   11, 1498,   12,   13,   14,   15,    8,    3,    0,\n",
       "             0,    0,    0],\n",
       "         [   2, 1493, 1497,   11,   16,   17,   18,    6, 1498,   12,   13,   19,\n",
       "            15,    8,    3],\n",
       "         [   2, 1493, 1497,   11, 1498,   12,   13,    4,   14,   15,    8,    3,\n",
       "             0,    0,    0],\n",
       "         [   2, 1493, 1497,   11,   20,   21, 1499,    8,    3,    0,    0,    0,\n",
       "             0,    0,    0],\n",
       "         [   2,    4,   22,    6, 1493, 1497,   11, 1499,    8,    3,    0,    0,\n",
       "             0,    0,    0]]),\n",
       " 'labels': tensor([[[4, 5],\n",
       "          [5, 4]],\n",
       " \n",
       "         [[4, 5],\n",
       "          [5, 4]],\n",
       " \n",
       "         [[4, 5],\n",
       "          [5, 4]],\n",
       " \n",
       "         [[4, 6],\n",
       "          [6, 4]],\n",
       " \n",
       "         [[4, 6],\n",
       "          [6, 4]],\n",
       " \n",
       "         [[4, 6],\n",
       "          [6, 4]],\n",
       " \n",
       "         [[4, 7],\n",
       "          [7, 4]],\n",
       " \n",
       "         [[4, 7],\n",
       "          [7, 4]]])}"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c5ce5b9604fc8445e4e5f27c24807def7cbbefbcd7dbec7e9c2f61ff534743b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
