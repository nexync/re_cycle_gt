{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the module\n",
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', '<ENT_1>', 'of', '<ENT_0>', '.']\n",
      "{'relations': [[['Aarhus', 'Airport'], 'cityServed', ['``', 'Aarhus', ',', 'Denmark', \"''\"]]], 'text': 'the <ENT_1> of <ENT_0> .', 'entities': [['``', 'Aarhus', ',', 'Denmark', \"''\"], ['Aarhus', 'Airport']]}\n"
     ]
    }
   ],
   "source": [
    "#setup vocabulary\n",
    "\n",
    "word_counter = Counter()\n",
    "for k, example in enumerate(raw_train):\n",
    "\tprint(example['text'].split())\n",
    "\tprint(example)\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([len(x['text'].split()) for x in raw_train])[int(0.95*len(raw_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = scan_data(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': <__main__.Vocab object at 0x0000023DC14D56D0>, 'entity': <__main__.Vocab object at 0x0000023DC14D51F0>, 'relation': <__main__.Vocab object at 0x0000023DC14D5400>}\n"
     ]
    }
   ],
   "source": [
    "for v in d.values():\n",
    "\tv.build()\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Total 2978['<PAD>', '<BOS>', '<EOS>', '<UNK>', '<ROOT>', 'the', '.', 'is', '<ENT_1>', '<ENT_0>']\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(d['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dgl\n",
    "import uuid\n",
    "import copy\n",
    "import random\n",
    "from transformers import BertTokenizer\n",
    "bert_type = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_type)\n",
    "\n",
    "NODE_TYPE = {'entity': 0, 'root': 1, 'relation':2}\n",
    "def pad(var_len_list, out_type='list', flatten=False):\n",
    "    #padding sequences\n",
    "    if flatten:\n",
    "        lens = [len(x) for x in var_len_list]\n",
    "        var_len_list = sum(var_len_list, [])\n",
    "    max_len = max([len(x) for x in var_len_list])\n",
    "    if out_type=='list':\n",
    "        if flatten:\n",
    "            return [x+['<PAD>']*(max_len-len(x)) for x in var_len_list], lens\n",
    "        else:\n",
    "            return [x+['<PAD>']*(max_len-len(x)) for x in var_len_list]\n",
    "    if out_type=='tensor':\n",
    "        if flatten:\n",
    "            return torch.stack([torch.cat([x, \\\n",
    "            torch.zeros([max_len-len(x)]+list(x.shape[1:])).type_as(x)], 0) for x in var_len_list], 0), lens\n",
    "        else:\n",
    "            return torch.stack([torch.cat([x, \\\n",
    "            torch.zeros([max_len-len(x)]+list(x.shape[1:])).type_as(x)], 0) for x in var_len_list], 0)\n",
    "\n",
    "def write_txt(batch, seqs, text_vocab):\n",
    "    # converting the prediction to real text.\n",
    "    ret = []\n",
    "    for b, seq in enumerate(seqs):\n",
    "        txt = []\n",
    "\n",
    "        for token in seq:\n",
    "            # copy the entity\n",
    "            if token>=len(text_vocab):\n",
    "                if (token-len(text_vocab))>=len(batch['raw_ent_text'][b]):\n",
    "                    print((token-len(text_vocab)), len(batch['raw_ent_text'][b]))\n",
    "                    tok = ['NO_ENT']\n",
    "                else:\n",
    "                    tok = batch['raw_ent_text'][b][token-len(text_vocab)]\n",
    "                    #tok = ['ENT_'+str(int(token-len(text_vocab)))+'_ENT'] \n",
    "                ent_text = tok \n",
    "                ent_text = filter(lambda x:x!='<PAD>', ent_text)\n",
    "                txt.extend(ent_text)\n",
    "            else:\n",
    "                if int(token) not in [text_vocab(x) for x in ['<PAD>', '<BOS>', '<EOS>']]:\n",
    "                    txt.append(text_vocab(int(token)))\n",
    "            if int(token) == text_vocab('<EOS>'):\n",
    "                break\n",
    "        ret.append([' '.join([str(x) for x in txt]).replace('<BOS>', '').replace('<EOS>', '')])\n",
    "    return ret\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, max_vocab=2**31, min_freq=-1, sp=['<PAD>', '<BOS>', '<EOS>', '<UNK>', '<ROOT>']):\n",
    "        self.i2s = []\n",
    "        self.s2i = {}\n",
    "        self.wf = {}\n",
    "        self.inv = {}\n",
    "        self.max_vocab, self.min_freq, self.sp = max_vocab, min_freq, copy.deepcopy(sp)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.i2s)\n",
    "\n",
    "    def __str__(self):\n",
    "        return 'Total ' + str(len(self.i2s)) + str(self.i2s[:10])\n",
    "\n",
    "    def merge(self, _vocab):\n",
    "        self.wf.update(_vocab.wf)\n",
    "        self.inv.update(_vocab.inv)\n",
    "        self.sp = list(set(self.sp + _vocab.sp))\n",
    "\n",
    "    def update(self, token, inv=False, sp=False):\n",
    "        if isinstance(token, list):\n",
    "            for t in token:\n",
    "                self.update(t, inv=inv, sp=sp)\n",
    "        else:\n",
    "            self.wf[token] = self.wf.get(token, 0) + 1\n",
    "            if inv:\n",
    "                self.wf[token+'_INV'] = self.wf.get(token+'_INV', 0) + 1\n",
    "                self.inv[token] = token+'_INV'\n",
    "            if sp and token not in self.sp:\n",
    "                self.sp.append(token)\n",
    "\n",
    "    def get_inv(self, idx):\n",
    "        return self.__call__(self.inv.get(self.i2s[idx], '<UNK>'))\n",
    "\n",
    "    def build(self):\n",
    "        self.i2s.extend(self.sp)\n",
    "        sort_kv = sorted(self.wf.items(), key=lambda x:x[1], reverse=True)\n",
    "        for k,v in sort_kv:\n",
    "            if len(self.i2s)<self.max_vocab and v>=self.min_freq and k not in self.sp:\n",
    "                self.i2s.append(k)\n",
    "        self.s2i.update(list(zip(self.i2s, range(len(self.i2s)))))\n",
    "\n",
    "    def __call__(self, x, ents=[]):\n",
    "        if isinstance(x, list):\n",
    "            return [self(y) for y in x]\n",
    "        if isinstance(x, int):\n",
    "            if x>=len(self.i2s):\n",
    "                return ents[int(x-len(self.i2s))]\n",
    "            return self.i2s[x]\n",
    "        else:\n",
    "            if x[0] == '<' and x[-1] == '>' and '_' in x:\n",
    "                try:\n",
    "                    t = len(self.s2i)+int(x.split('_')[1][:-1])\n",
    "                except:\n",
    "                    print(x)\n",
    "                return len(self.s2i)+int(x.split('_')[1][:-1])\n",
    "            return self.s2i.get(x, self.s2i['<UNK>'])\n",
    "\n",
    "def scan_data(datas, vocab=None, sp=False):\n",
    "    MF = -1\n",
    "\n",
    "    if vocab is None:\n",
    "        vocab = {'text':Vocab(min_freq=MF), 'entity':Vocab(min_freq=MF), 'relation':Vocab()}\n",
    "    for data in datas:\n",
    "        vocab['text'].update(data['text'].split(), sp=sp)\n",
    "        vocab['entity'].update(sum(data['entities'], []), sp=sp)\n",
    "        vocab['relation'].update([x[1] for x in data['relations']], inv=True)\n",
    "    return vocab\n",
    "\n",
    "def get_graph(ent_len, rel_len, adj_edges):\n",
    "    graph = dgl.DGLGraph()\n",
    "\n",
    "    graph.add_nodes(ent_len,\n",
    "                    {'type': torch.ones(ent_len) * NODE_TYPE['entity']})\n",
    "    graph.add_nodes(1, {'type': torch.ones(1) * NODE_TYPE['root']})\n",
    "    graph.add_nodes(rel_len * 2,\n",
    "                    {'type': torch.ones(rel_len * 2) * NODE_TYPE['relation']})\n",
    "    graph.add_edges(ent_len, torch.arange(ent_len))\n",
    "    graph.add_edges(torch.arange(ent_len), ent_len)\n",
    "    graph.add_edges(torch.arange(ent_len + 1 + rel_len * 2),\n",
    "                    torch.arange(ent_len + 1 + rel_len * 2))\n",
    "\n",
    "    if len(adj_edges) > 0:\n",
    "        graph.add_edges(*list(map(list, zip(*adj_edges))))\n",
    "    return graph\n",
    "\n",
    "def build_graph(ent_len, relations):\n",
    "    rel_len = len(relations)\n",
    "\n",
    "    adj_edges = []\n",
    "    for i, r in enumerate(relations):\n",
    "        st_ent, rt, ed_ent = r\n",
    "        # according to the edge_softmax operator, we need to reverse the graph\n",
    "        adj_edges.append([ent_len+1+2*i, st_ent])\n",
    "        adj_edges.append([ed_ent, ent_len+1+2*i])\n",
    "        adj_edges.append([ent_len+1+2*i+1, ed_ent])\n",
    "        adj_edges.append([st_ent, ent_len+1+2*i+1])\n",
    "\n",
    "    graph = get_graph(ent_len, rel_len, adj_edges)\n",
    "    return graph\n",
    "\n",
    "class Example(object):\n",
    "    def __init__(self, data, vocab):\n",
    "        self.uuid = uuid.uuid4()\n",
    "        self.vocab = vocab\n",
    "        self.text = [vocab['text'](x) for x in data['text'].split()]\n",
    "        self.entities = [vocab['entity'](x) for x in data['entities']]\n",
    "        self.relations = []\n",
    "        for r in data['relations']:\n",
    "            e1, e2 = vocab['entity'](r[0]), vocab['entity'](r[2])\n",
    "            rel = vocab['relation'](r[1])\n",
    "            e1, e2 = self.entities.index(e1), self.entities.index(e2)\n",
    "            self.relations.append([e1, rel, e2])\n",
    "\n",
    "        self.graph = None\n",
    "        self.graph = build_graph(len(self.entities), self.relations)\n",
    "        self.id = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return '\\n'.join(\n",
    "            [str(k) + ':\\t' + str(v) for k, v in self.__dict__.items()])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "\n",
    "    def get(self):\n",
    "        if hasattr(self, '_cached_tensor') and False:\n",
    "            return self._cached_tensor\n",
    "        else:\n",
    "            vocab = self.vocab\n",
    "            ret = {}\n",
    "            ret['text'] = [vocab['text']('<BOS>')] + self.text + [vocab['text']('<EOS>')]\n",
    "            ret['ent_text'] = [[vocab['entity']('<BOS>')] + x + [vocab['entity']('<EOS>')] for x in self.entities]\n",
    "            ret['relation'] = [vocab['relation']('<ROOT>')] + sum([[x[1], vocab['relation'].get_inv(x[1])] for x in self.relations], [])\n",
    "            ret['raw_relation'] = self.relations\n",
    "            ret['graph'] = self.graph\n",
    "            ret['uuid'] = self.uuid\n",
    "\n",
    "            self._cached_tensor = ret\n",
    "            return self._cached_tensor\n",
    "\n",
    "class DataPool(object):\n",
    "    def __init__(self):\n",
    "        self.pool = []\n",
    "        self.types={}\n",
    "\n",
    "    def add(self, data, _type='gold'):\n",
    "        self.pool.append(data)\n",
    "        if _type not in self.types:\n",
    "            self.types[_type] = [] \n",
    "        self.types[_type].append(len(self.pool)-1)\n",
    "\n",
    "    def remove(self, _type, _id):\n",
    "        del self.types[_type][_id]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pool)\n",
    "\n",
    "    def get_len(self, _type):\n",
    "        return len(self.types.get(_type, []))\n",
    "\n",
    "    def draw_with_type(self, batch_size=32, shuffle=True, _type='gold', tot=1.1):\n",
    "        batch = []\n",
    "        from copy import deepcopy\n",
    "        if shuffle:\n",
    "            random.shuffle(self.types[_type])\n",
    "        for i,idx in enumerate(self.types[_type]):\n",
    "            batch.append(deepcopy(self.pool[idx]))\n",
    "            if len(batch)>=batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "            if i>tot*len(self.types[_type]):\n",
    "                break\n",
    "        if len(batch)>0:\n",
    "            yield batch\n",
    "\n",
    "def batch2tensor_g2t(datas, device, vocab):\n",
    "    # raw batch to tensor \n",
    "    ret = {}\n",
    "    ret['ent_len'] = [len(x['ent_text']) for x in datas]\n",
    "    ents = [vocab['entity'](x['ent_text']) for x in datas]\n",
    "    ret['raw_ent_text'] = ents\n",
    "    ret['text'] = pad([torch.LongTensor(x['text']) for x in datas], 'tensor').to(device)\n",
    "    ret['tgt'] = ret['text'][:,1:]\n",
    "    ret['text'] = ret['text'][:,:-1]\n",
    "    ent_text = sum([[torch.LongTensor(y) for y in x['ent_text']] for x in datas], [])\n",
    "    ret['ent_text'] = pad(ent_text, 'tensor').to(device)\n",
    "    ret['rel'] = pad([torch.LongTensor(x['relation']) for x in datas], 'tensor').to(device)\n",
    "    ret['graph'] = dgl.batch([x['graph'] for x in datas]).to(device)\n",
    "    return ret\n",
    "\n",
    "\n",
    "def tensor2data_g2t(old_data, pred):\n",
    "    # construct synthetic data based on the old data and model prediction\n",
    "    new_data = {}\n",
    "    new_data['text'] = pred\n",
    "    new_data['ent_text'] = old_data['ent_text']\n",
    "    new_data['relation'] = old_data['relation']\n",
    "    new_data['raw_relation'] = old_data['raw_relation']\n",
    "    new_data['graph'] = old_data['graph']\n",
    "    new_data['uuid'] = old_data['uuid']\n",
    "    return new_data\n",
    "\n",
    "def tensor2data_t2g(old_data, pred, vocab):\n",
    "    # construct synthetic data based on the old data and model prediction\n",
    "    new_data = {}\n",
    "    new_data['text'] = old_data['text']\n",
    "    new_data['ent_text'] = old_data['ent_text']\n",
    "    new_data['relation'] = [vocab['relation']('<ROOT>')] + sum([[x[1], vocab['relation'].get_inv(x[1])] for x in pred], [])\n",
    "    new_data['graph'] = build_graph(len(new_data['ent_text']), pred)\n",
    "    new_data['uuid'] = old_data['uuid']\n",
    "    return new_data\n",
    "\n",
    "def batch2tensor_t2g(datas, device, vocab, add_inp=False):\n",
    "    # raw batch to tensor, we use the Bert tokenizer for the T2G model\n",
    "    ret = {}\n",
    "    ent_pos = []\n",
    "    text = []\n",
    "    tgt = []\n",
    "    MAX_ENT = 100\n",
    "    ent_len = 1\n",
    "    for data in datas:\n",
    "        ents = [vocab['entity'](x) for x in data['ent_text']]\n",
    "        st, ed = [], []\n",
    "        cstr = ''\n",
    "        ent_order = []\n",
    "        for i, t in enumerate(data['text']):\n",
    "            if t>=len(vocab['text']):\n",
    "                ff = (t-len(vocab['text'])) not in ent_order\n",
    "                if ff:\n",
    "                    st.append(len(cstr))\n",
    "                cstr += ' '.join([x for x in vocab['text'](t, ents) if x[0]!='<' and x[-1]!='>'])\n",
    "                if ff:\n",
    "                    ent_order.append(t-len(vocab['text']))\n",
    "                    ed.append(len(cstr))\n",
    "            else:\n",
    "                if vocab['text'](t)[0]=='<':\n",
    "                    continue\n",
    "                cstr += vocab['text'](t)\n",
    "            cstr += '' if i==len(data['text'])-1 else ' '\n",
    "        if add_inp:\n",
    "            cstr += ' ' + ' '.join([' '.join(e) for e in ents])\n",
    "        tok_abs = [\"[CLS]\"] + tokenizer.tokenize(cstr) + [\"[SEP]\"]\n",
    "        _ent_pos = []\n",
    "        for s,e in zip(st, ed):\n",
    "            guess_start = s - cstr[:s].count(\" \") + 5\n",
    "            guess_end   = e - cstr[:e].count(\" \") + 5 \n",
    "\n",
    "            new_s = -1\n",
    "            new_e = -1\n",
    "            l = 0\n",
    "            r = 0\n",
    "            for i in range(len(tok_abs)):\n",
    "                l = r\n",
    "                r = l + len(tok_abs[i]) - tok_abs[i].count(\"##\")*2\n",
    "                if l <= guess_start and guess_start < r:\n",
    "                    new_s = i\n",
    "                if l <= guess_end and guess_end < r:\n",
    "                    new_e = i\n",
    "            _ent_pos.append((new_s, new_e))\n",
    "        _order_ent_pos = []\n",
    "        for _e in range(len(ents)):\n",
    "            if _e in ent_order:\n",
    "                idx = ent_order.index(_e)\n",
    "                _order_ent_pos.append(_ent_pos[idx])\n",
    "            else:\n",
    "                idx = 0\n",
    "                _order_ent_pos.append((0, 1))\n",
    "\n",
    "        ent_pos.append(_order_ent_pos)\n",
    "        text.append(tokenizer.convert_tokens_to_ids(tok_abs))\n",
    "        _tgt = torch.zeros(MAX_ENT, MAX_ENT)\n",
    "        _tgt[:len(_ent_pos), :len(_ent_pos)] += 3 # <UNK>\n",
    "        for _e1, _r, _e2 in data['raw_relation']:\n",
    "            if _e1 not in ent_order or _e2 not in ent_order: # the synthetic data may lose some entities\n",
    "                continue\n",
    "            _tgt[_e1, _e2] = _r\n",
    "        tgt.append(_tgt)\n",
    "        ent_len = max(ent_len, len(_order_ent_pos))\n",
    "    ret['sents'] = pad([torch.LongTensor(x) for x in text], 'tensor').to(device)\n",
    "    ret['ents'] = ent_pos\n",
    "    ret['tgt'] = torch.stack(tgt,0)[:,:ent_len,:ent_len].long().to(device)\n",
    "    return ret\n",
    "\n",
    "def fill_pool(pool, vocab, datas, _type):\n",
    "    for data in datas:\n",
    "        ex = Example(data, vocab).get()\n",
    "        pool.add(ex, _type)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e04db361a4f8367a2d031f37306cd7dd06d0ce8a25d39dd9e4b8b2f9ddb79174"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('recyclegt': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
