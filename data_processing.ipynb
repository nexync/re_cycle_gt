{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the module\n",
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabCategory():\n",
    "\tdef __init__(self):\n",
    "\t\tself.wordlist = []\n",
    "\t\tself.word2idx = {}\n",
    "\t\tself.wordfreq = Counter()\n",
    "\n",
    "class Vocabulary():\n",
    "\tdef __init__(self):\n",
    "\t\tprint(\"Creating empty vocabulary object\")\n",
    "\t\tself.text = VocabCategory()\n",
    "\t\tself.entities = VocabCategory()\n",
    "\t\tself.relations = VocabCategory()\n",
    "\tdef parseSentence(self, raw_json_sentence):\n",
    "\t\tfor relation in raw_json_sentence['relations']: #Relation parsing here\n",
    "\t\t\tassert len(relation) == 3, \"CHECK THIS!\"\n",
    "\t\t\tif relation[1] not in self.relations.word2idx:\n",
    "\t\t\t\tself.relations.word2idx[relation[1]] = len(self.relations.wordlist)+1\n",
    "\t\t\t\tself.relations.wordlist.append(relation[1])\n",
    "\t\t\tself.relations.wordfreq.update({relation[1]: 1})\n",
    "\t\t\n",
    "\t\tfor word in raw_json_sentence['text'].split(): #Word parsing here\n",
    "\t\t\tif word not in self.text.word2idx:\n",
    "\t\t\t\tself.text.word2idx[word] = len(self.text.wordlist)\n",
    "\t\t\t\tself.text.wordlist.append(word)\n",
    "\t\tself.text.wordfreq += Counter(raw_json_sentence['text'].split())\n",
    "\n",
    "\t\tfor entity in raw_json_sentence['entities']:\n",
    "\t\t\tfor e in entity:\n",
    "\t\t\t\tif e not in self.entities.word2idx:\n",
    "\t\t\t\t\tself.entities.word2idx[e] = len(self.entities.wordlist)\n",
    "\t\t\t\t\tself.entities.wordlist.append(e)\n",
    "\t\t\tself.entities.wordfreq += Counter(entity)\n",
    "\t\n",
    "\tdef parseText(self, raw_json):\n",
    "\t\tfor raw_sentence in raw_json:\n",
    "\t\t\tself.parseSentence(raw_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating empty vocabulary object\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.parseText(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relations': [[['Abilene', 'Regional', 'Airport'],\n",
       "   'runwayLength',\n",
       "   ['2195.0']]],\n",
       " 'text': 'the runway length of <ENT_0> is <ENT_1> .',\n",
       " 'entities': [['Abilene', 'Regional', 'Airport'], ['2195.0']]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train[54]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AbileneRegionalAirport': 0, '2195.0': 1}\n",
      "[['Abilene', 'Regional', 'Airport'], 'runwayLength', ['2195.0']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 5],\n",
       "        [5, 0]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text2Relation(vocab, raw_json_sentence):\n",
    "\tl = len(raw_json_sentence['entities'])\n",
    "\tret = torch.zeros((l,l), dtype = torch.long)\n",
    "\tentitydict = {}\n",
    "\tfor i, entity in enumerate(raw_json_sentence['entities']):\n",
    "\t\tentitydict[\"\".join(entity)] = i\n",
    "\tprint(entitydict)\n",
    "\tfor relation in raw_json_sentence['relations']:\n",
    "\t\tprint(relation)\n",
    "\t\tind1 = entitydict[\"\".join(relation[0])]\n",
    "\t\tind2 = entitydict[\"\".join(relation[2])]\n",
    "\t\tret[ind1][ind2] = ret[ind2][ind1] = vocab.relations.word2idx[relation[1]]\n",
    "\treturn ret\n",
    "\n",
    "text2Relation(vocab, raw_train[54])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity2Indices(vocab, entity):\n",
    "\ttemp = torch.zeros(len(entity), dtype = torch.long)\n",
    "\tfor ind, word in enumerate(entity):\n",
    "\t\tif word not in vocab.entities.word2idx:\n",
    "\t\t\ttemp[ind] = -1\n",
    "\t\telse:\n",
    "\t\t\ttemp[ind] = vocab.entities.word2idx[word]\n",
    "\treturn temp\n",
    "\t\t\n",
    "def text2Indices(vocab, text):\n",
    "\ttemp = torch.zeros(len(text.split()), dtype=torch.long)\n",
    "\tfor ind, word in enumerate(text.split()):\n",
    "\t\tif word not in vocab.text.word2idx:\n",
    "\t\t\ttemp[ind] = -1\n",
    "\t\telse:\n",
    "\t\t\ttemp[ind] = vocab.text.word2idx[word]\n",
    "\treturn temp\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relations': [[['Abilene', ',', 'Texas'],\n",
       "   'isPartOf',\n",
       "   ['Jones', 'County', ',', 'Texas']]],\n",
       " 'text': '<ENT_0> is part of <ENT_1> .',\n",
       " 'entities': [['Abilene', ',', 'Texas'], ['Jones', 'County', ',', 'Texas']]}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train[27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1484"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatTextEntities(vocab, raw_json_sentence, entity_indices):\n",
    "\tsent = text2Indices(vocab, raw_json_sentence['text'])\n",
    "\tmodified_input = torch.LongTensor([0])\n",
    "\tlbound = 0\n",
    "\tentity_locations = []\n",
    "\tadditional_words = 0\n",
    "\tfor index, value in enumerate(sent):\n",
    "\t\tif value.item() in entity_indices:\n",
    "\t\t\ttemp = entity2Indices(vocab, raw_json_sentence['entities'][entity_indices[value.item()]])\n",
    "\t\t\ttemp += len(vocab.text.wordlist)\n",
    "\t\t\tmodified_input = torch.cat((modified_input, sent[lbound:index], temp), dim = 0)\n",
    "\t\t\tentity_locations.append((index + additional_words, index + additional_words + len(temp)))\n",
    "\t\t\tadditional_words += len(temp) - 1\n",
    "\t\t\tlbound = index + 1\n",
    "\tmodified_input = torch.cat((modified_input, sent[lbound:]), dim = 0)[1:]\n",
    "\n",
    "\treturn modified_input, entity_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: 0, 1: 1, 721: 2, 959: 3, 1131: 4, 1343: 5, 1445: 6, 1471: 7}\n",
      "['<ENT_0>', '<ENT_1>', '<ENT_2>', '<ENT_3>', '<ENT_4>', '<ENT_5>', '<ENT_6>', '<ENT_7>']\n"
     ]
    }
   ],
   "source": [
    "entity_indices = {}\n",
    "i = 0\n",
    "while True:\n",
    "\tif '<ENT_' + str(i) + '>' in vocab.text.word2idx:\n",
    "\t\tentity_indices[(vocab.text.word2idx['<ENT_' + str(i) + '>'])] = i\n",
    "\t\ti += 1\n",
    "\telse:\n",
    "\t\tbreak\n",
    "\n",
    "print(entity_indices)\n",
    "print([vocab.text.wordlist[k] for k in entity_indices])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e04db361a4f8367a2d031f37306cd7dd06d0ce8a25d39dd9e4b8b2f9ddb79174"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('recyclegt': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
