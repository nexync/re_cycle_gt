{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the module\n",
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relations': [[['Aarhus', 'Airport'],\n",
       "   'cityServed',\n",
       "   ['``', 'Aarhus', ',', 'Denmark', \"''\"]]],\n",
       " 'text': 'the <ENT_1> of <ENT_0> .',\n",
       " 'entities': [['``', 'Aarhus', ',', 'Denmark', \"''\"], ['Aarhus', 'Airport']]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VocabCategory():\n",
    "\tdef __init__(self):\n",
    "\t\tself.wordlist = []\n",
    "\t\tself.word2idx = {}\n",
    "\t\tself.wordfreq = Counter()\n",
    "\n",
    "class Vocabulary():\n",
    "\tdef __init__(self):\n",
    "\t\tprint(\"Creating empty vocabulary object\")\n",
    "\t\tself.text = VocabCategory()\n",
    "\t\tself.entities = VocabCategory()\n",
    "\t\tself.relations = VocabCategory()\n",
    "\n",
    "\t\tself.relations.word2idx[\"<NO_RELATION>\"] = len(self.relations.wordlist) # no relation token for relations vocab\n",
    "\t\tself.relations.wordlist.append(\"<NO_RELATION>\")\n",
    "\n",
    "\t\tself.init_vocab(self.text)\n",
    "\t\tself.init_vocab(self.entities)\n",
    "\t\tself.init_vocab(self.relations)\n",
    "\t\t\n",
    "\t# initializes UNK, SOS, EOS, and EMPTY tokens\n",
    "\tdef init_vocab(self, vocab_category):\n",
    "\t\ttokens = [\"<UNK>\", \"<SOS>\", \"<EOS>\", \"<EMPTY>\"]\n",
    "\n",
    "\t\tfor token in tokens:\n",
    "\t\t\tvocab_category.word2idx[token] = len(vocab_category.wordlist)\n",
    "\t\t\tvocab_category.wordlist.append(token)\n",
    "\t\t# vocab_category.word2idx[\"<UNK>\"] = len(vocab_category.wordlist)\n",
    "\t\t# vocab_category.wordlist.append(\"<UNK>\")\n",
    "\t\t# vocab_category.word2idx[\"<SOS>\"] = len(vocab_category.wordlist)\n",
    "\t\t# vocab_category.wordlist.append(\"<SOS>\")\n",
    "\t\t# vocab_category.word2idx[\"<EOS>\"] = len(vocab_category.wordlist)\n",
    "\t\t# vocab_category.wordlist.append(\"<EOS>\")\n",
    "\t\t# vocab_category.word2idx[\"<EMPTY>\"] = len(vocab_category.wordlist)\n",
    "\t\t# vocab_category.wordlist.append(\"<EMPTY>\")\n",
    "\t\t\n",
    "\n",
    "\tdef parseSentence(self, raw_json_sentence):\n",
    "\t\tfor relation in raw_json_sentence['relations']: #Relation parsing here\n",
    "\t\t\tassert len(relation) == 3, \"CHECK THIS!\"\n",
    "\t\t\tif relation[1] not in self.relations.word2idx:\n",
    "\t\t\t\tself.relations.word2idx[relation[1]] = len(self.relations.wordlist)\n",
    "\t\t\t\tself.relations.wordlist.append(relation[1])\n",
    "\t\t\tself.relations.wordfreq.update({relation[1]: 1})\n",
    "\t\t\n",
    "\t\tfor word in raw_json_sentence['text'].split(): #Word parsing here\n",
    "\t\t\tif word not in self.text.word2idx:\n",
    "\t\t\t\tself.text.word2idx[word] = len(self.text.wordlist)\n",
    "\t\t\t\tself.text.wordlist.append(word)\n",
    "\t\tself.text.wordfreq += Counter(raw_json_sentence['text'].split())\n",
    "\n",
    "\t\tfor entity in raw_json_sentence['entities']:\n",
    "\t\t\tfor e in entity:\n",
    "\t\t\t\tif e not in self.entities.word2idx:\n",
    "\t\t\t\t\tself.entities.word2idx[e] = len(self.entities.wordlist)\n",
    "\t\t\t\t\tself.entities.wordlist.append(e)\n",
    "\t\t\tself.entities.wordfreq += Counter(entity)\n",
    "\t\n",
    "\tdef parseText(self, raw_json):\n",
    "\t\tfor raw_sentence in raw_json:\n",
    "\t\t\tself.parseSentence(raw_sentence)\n",
    "\t\tprint(\"Finished Parsing Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating empty vocabulary object\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab.parseText(raw_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'relations': [[['Abilene', 'Regional', 'Airport'], 'runwayLength', ['2195.0']]], 'text': 'the runway length of <ENT_0> is <ENT_1> .', 'entities': [['Abilene', 'Regional', 'Airport'], ['2195.0']]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 9],\n",
       "        [9, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def entity2Indices(vocab, entity):\n",
    "\ttemp = torch.zeros(len(entity), dtype = torch.long)\n",
    "\tfor ind, word in enumerate(entity):\n",
    "\t\tif word not in vocab.entities.word2idx:\n",
    "\t\t\ttemp[ind] = vocab.entities.word2idx[\"<UNK>\"]\n",
    "\t\telse:\n",
    "\t\t\ttemp[ind] = vocab.entities.word2idx[word]\n",
    "\treturn temp\n",
    "\t\t\n",
    "def text2Indices(vocab, text):\n",
    "\ttemp = torch.zeros(len(text.split()) + 2, dtype=torch.long)\n",
    "\ttemp[0] = vocab.text.word2idx[\"<SOS>\"]\n",
    "\tfor ind, word in enumerate(text.split()):\n",
    "\t\tif word not in vocab.text.word2idx:\n",
    "\t\t\ttemp[ind + 1] = vocab.text.word2idx[\"<UNK>\"]\n",
    "\t\telse:\n",
    "\t\t\ttemp[ind + 1] = vocab.text.word2idx[word]\n",
    "\ttemp[-1] = vocab.text.word2idx[\"<EOS>\"]\n",
    "\treturn temp\n",
    "\n",
    "def relation2Indices(vocab, raw_json_sentence):\n",
    "\tl = len(raw_json_sentence['entities'])\n",
    "\tret = torch.zeros((l,l), dtype = torch.long)\n",
    "\tentitydict = {}\n",
    "\tfor i, entity in enumerate(raw_json_sentence['entities']):\n",
    "\t\tentitydict[\"\".join(entity)] = i\n",
    "\tfor relation in raw_json_sentence['relations']:\n",
    "\t\tind1 = entitydict[\"\".join(relation[0])]\n",
    "\t\tind2 = entitydict[\"\".join(relation[2])]\n",
    "\t\tret[ind1][ind2] = ret[ind2][ind1] = vocab.relations.word2idx[relation[1]]\n",
    "\treturn ret\n",
    "\n",
    "print(raw_train[54])\n",
    "relation2Indices(vocab, raw_train[54])\n",
    "\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatTextEntities(vocab, raw_json_sentence, entity_indices):\n",
    "\tsent = text2Indices(vocab, raw_json_sentence['text'])\n",
    "\tmodified_input = torch.LongTensor([0])\n",
    "\tlbound = 0\n",
    "\tentity_locations = []\n",
    "\tadditional_words = 0\n",
    "\tfor index, value in enumerate(sent):\n",
    "\t\tif value.item() in entity_indices:\n",
    "\t\t\ttemp = entity2Indices(vocab, raw_json_sentence['entities'][entity_indices[value.item()]])\n",
    "\t\t\ttemp += len(vocab.text.wordlist)\n",
    "\t\t\tmodified_input = torch.cat((modified_input, sent[lbound:index], temp), dim = 0)\n",
    "\t\t\tentity_locations.append([index + additional_words, index + additional_words + len(temp)])\n",
    "\t\t\tadditional_words += len(temp) - 1\n",
    "\t\t\tlbound = index + 1\n",
    "\tmodified_input = torch.cat((modified_input, sent[lbound:]), dim = 0)[1:]\n",
    "\treturn modified_input, torch.tensor(entity_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{7: 0, 5: 1, 725: 2, 963: 3, 1135: 4, 1347: 5, 1449: 6, 1475: 7}\n",
      "['<ENT_0>', '<ENT_1>', '<ENT_2>', '<ENT_3>', '<ENT_4>', '<ENT_5>', '<ENT_6>', '<ENT_7>']\n"
     ]
    }
   ],
   "source": [
    "def getEntityIndices(vocab):\n",
    "\tentity_indices = {}\n",
    "\ti = 0\n",
    "\twhile True:\n",
    "\t\tif '<ENT_' + str(i) + '>' in vocab.text.word2idx:\n",
    "\t\t\tentity_indices[(vocab.text.word2idx['<ENT_' + str(i) + '>'])] = i\n",
    "\t\t\ti += 1\n",
    "\t\telse:\n",
    "\t\t\treturn entity_indices\n",
    "\n",
    "\n",
    "entity_indices = getEntityIndices(vocab)\n",
    "print(entity_indices)\n",
    "print([vocab.text.wordlist[k] for k in entity_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class text2GraphDataset(Dataset):\n",
    "\tdef __init__(self, raw_json_file):\n",
    "\t\tprint(\"Creating custom dataset for T2G task\")\n",
    "\t\t\n",
    "\t\tself.vocab = Vocabulary()\n",
    "\t\tself.vocab.parseText(raw_json_file)\n",
    "\t\t\n",
    "\t\tself.inputs = []\n",
    "\t\tself.labels = []\n",
    "\t\t\n",
    "\t\tself.entity_indices = getEntityIndices(self.vocab)\n",
    "\n",
    "\t\tfor raw_json_sentence in raw_json_file:\n",
    "\t\t\tself.labels.append(relation2Indices(self.vocab, raw_json_sentence))\n",
    "\t\t\tself.inputs.append(concatTextEntities(self.vocab, raw_json_sentence, self.entity_indices))\n",
    "\n",
    "\t\tprint(\"Finished processing raw json file\")\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.inputs)\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\treturn self.inputs[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating custom dataset for T2G task\n",
      "Creating empty vocabulary object\n",
      "Finished Parsing Text\n",
      "Finished processing raw json file\n"
     ]
    }
   ],
   "source": [
    "dataset = text2GraphDataset(raw_json_file = raw_train)\n",
    "train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[   1,  311,   21, 3747,   37,    4, 2178, 1964, 2179,   83,   20,   16,\n",
      "         2186, 2187, 1494, 1930,    8,    2]]), tensor([[[ 3,  4],\n",
      "         [ 6,  9],\n",
      "         [12, 16]]])]\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(train_dataloader))\n",
    "print(inputs)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c5ce5b9604fc8445e4e5f27c24807def7cbbefbcd7dbec7e9c2f61ff534743b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
