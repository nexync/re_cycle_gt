{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processing as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating empty vocabulary object\nFinished Parsing Text\nCreating custom dataset for T2G task\nCreating empty vocabulary object\nFinished Parsing Text\nFinished processing raw json file\n"
    }
   ],
   "source": [
    "# importing the module\n",
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)\n",
    "\n",
    "vocab = dp.Vocabulary()\n",
    "vocab.parseText(raw_train)\n",
    "\n",
    "dataset = dp.text2GraphDataset(raw_json_file = raw_train)\n",
    "dataloader = dp.getBatches(vocab, dataset, batch_size = 8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[tensor([1493, 1497]), tensor([1492, 1493, 1494, 1495, 1496])]\n[tensor([1493, 1497]), tensor([1492, 1493, 1494, 1495, 1496])]\n[tensor([1493, 1497]), tensor([1493])]\n[tensor([1493, 1497]), tensor([1498])]\n[tensor([1493, 1497]), tensor([1498])]\n[tensor([1493, 1497]), tensor([1498])]\n[tensor([1493, 1497]), tensor([1499])]\n[tensor([1493, 1497]), tensor([1499])]\n"
    }
   ],
   "source": [
    "for (x,y) in zip(dataloader[0]['text'],dataloader[0]['entity_inds']):\n",
    "\tprint([x[z[0]:z[1]] for z in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_types = len(vocab.entities.wordlist) + len(vocab.text.wordlist)\n",
    "rel_types = len(vocab.relations.wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'entity_inds': [[(2, 4), (5, 10)],\n  [(1, 3), (7, 12)],\n  [(1, 3), (7, 8)],\n  [(1, 3), (4, 5)],\n  [(1, 3), (8, 9)],\n  [(1, 3), (4, 5)],\n  [(1, 3), (6, 7)],\n  [(4, 6), (7, 8)]],\n 'text_lengths': [12, 14, 10, 11, 15, 12, 9, 10],\n 'entity_lengths': [2, 2, 2, 2, 2, 2, 2, 2],\n 'text': tensor([[   2,    4, 1493, 1497,    6, 1492, 1493, 1494, 1495, 1496,    8,    3,\n             0,    0,    0],\n         [   2, 1493, 1497,    9,    4,   10,    6, 1492, 1493, 1494, 1495, 1496,\n             8,    3,    0],\n         [   2, 1493, 1497,    9,    4,   10,    6, 1493,    8,    3,    0,    0,\n             0,    0,    0],\n         [   2, 1493, 1497,   11, 1498,   12,   13,   14,   15,    8,    3,    0,\n             0,    0,    0],\n         [   2, 1493, 1497,   11,   16,   17,   18,    6, 1498,   12,   13,   19,\n            15,    8,    3],\n         [   2, 1493, 1497,   11, 1498,   12,   13,    4,   14,   15,    8,    3,\n             0,    0,    0],\n         [   2, 1493, 1497,   11,   20,   21, 1499,    8,    3,    0,    0,    0,\n             0,    0,    0],\n         [   2,    4,   22,    6, 1493, 1497,   11, 1499,    8,    3,    0,    0,\n             0,    0,    0]]),\n 'labels': tensor([[[4, 5],\n          [5, 4]],\n \n         [[4, 5],\n          [5, 4]],\n \n         [[4, 5],\n          [5, 4]],\n \n         [[4, 6],\n          [6, 4]],\n \n         [[4, 6],\n          [6, 4]],\n \n         [[4, 6],\n          [6, 4]],\n \n         [[4, 7],\n          [7, 4]],\n \n         [[4, 7],\n          [7, 4]]])}"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "dataloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[[[-6.0135, -5.8261, -6.2549,  ..., -6.1588, -5.7649, -5.5606],\n          [-5.3612, -5.8228, -6.2137,  ..., -5.6133, -5.3899, -6.0846]],\n\n         [[-6.1191, -6.3883, -6.9742,  ..., -6.0371, -5.3482, -6.7017],\n          [-5.8671, -4.4141, -6.6187,  ..., -5.7058, -5.9226, -5.3751]]],\n\n\n        [[[-5.8942, -5.7228, -5.9620,  ..., -6.1625, -5.5074, -6.6184],\n          [-6.2515, -6.4347, -6.7804,  ..., -5.3424, -6.0351, -7.0967]],\n\n         [[-5.7410, -6.1617, -6.5203,  ..., -4.9812, -5.9386, -6.5096],\n          [-6.4004, -5.8797, -6.4620,  ..., -5.7474, -5.8288, -5.9985]]],\n\n\n        [[[-6.5456, -5.9010, -6.6531,  ..., -4.8142, -5.2187, -5.4348],\n          [-6.2245, -5.9202, -7.1903,  ..., -5.6096, -5.7418, -5.5788]],\n\n         [[-6.4174, -6.5166, -6.9035,  ..., -5.0348, -5.3933, -5.6015],\n          [-5.7416, -6.4307, -6.0652,  ..., -5.5198, -5.5212, -6.7901]]],\n\n\n        ...,\n\n\n        [[[-5.8710, -5.2130, -5.7292,  ..., -6.1913, -5.8534, -6.5523],\n          [-6.3254, -5.6742, -6.2095,  ..., -6.1744, -5.7865, -6.5040]],\n\n         [[-6.0691, -5.9335, -6.5263,  ..., -4.8001, -5.3912, -6.3463],\n          [-6.1906, -5.5482, -6.7386,  ..., -6.5632, -5.5661, -5.7229]]],\n\n\n        [[[-6.1289, -5.5974, -5.3703,  ..., -5.9320, -5.7631, -6.3432],\n          [-5.2935, -5.3073, -6.2208,  ..., -6.2556, -6.2208, -5.5047]],\n\n         [[-5.6651, -5.5297, -6.6779,  ..., -5.7230, -6.0682, -5.6651],\n          [-5.0901, -5.8739, -7.5995,  ..., -6.4855, -5.5344, -5.4570]]],\n\n\n        [[[-5.9830, -5.6056, -6.0222,  ..., -5.6389, -6.3161, -6.6330],\n          [-6.1274, -6.1267, -6.4464,  ..., -5.4467, -6.2718, -5.8275]],\n\n         [[-5.8201, -6.4153, -7.3789,  ..., -5.9429, -6.0104, -5.3374],\n          [-5.6176, -5.5262, -6.8956,  ..., -6.3631, -5.6358, -5.8215]]]],\n       grad_fn=<LogSoftmaxBackward>)"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "model = ModelLSTM(inp_types, rel_types, 100)\n",
    "\n",
    "#dataloader[0]\n",
    "model.forward(dataloader[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLSTM(nn.Module):\n",
    "\tdef __init__(self, input_types, relation_types, model_dim, dropout = 0.5):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.word_types = input_types\n",
    "\t\tself.relation_types = relation_types\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.model_dim = model_dim\n",
    "\n",
    "\t\tself.emb = nn.Embedding(input_types, self.model_dim) # 40000 because we use the Bert tokenizer\n",
    "\t\tself.lstm = nn.LSTM(self.model_dim, self.model_dim//2, batch_first=True, bidirectional=True, num_layers=2)\n",
    "\t\tself.relation_layer1 = nn.Linear(self.model_dim , self.model_dim)\n",
    "\t\tself.relation_layer2 = nn.Linear(self.model_dim , self.model_dim)\n",
    "\t\tself.drop = nn.Dropout(self.dropout)\n",
    "\t\tself.projection = nn.Linear(self.model_dim , self.model_dim)\n",
    "\t\tself.decoder = nn.Linear(self.model_dim , self.relation_types)\n",
    "\t\tself.layer_norm = nn.LayerNorm(self.model_dim)\n",
    "\n",
    "\t\tself.init_params()\n",
    "\n",
    "\tdef init_params(self):\n",
    "\t\tnn.init.xavier_normal_(self.relation_layer1.weight.data)\n",
    "\t\tnn.init.xavier_normal_(self.relation_layer2.weight.data)\n",
    "\t\tnn.init.xavier_normal_(self.projection.weight.data)\n",
    "\t\tnn.init.xavier_normal_(self.decoder.weight.data)\n",
    "\n",
    "\t\tnn.init.constant_(self.relation_layer1.bias.data , 0)\n",
    "\t\tnn.init.constant_(self.relation_layer2.bias.data , 0)\n",
    "\t\tnn.init.constant_(self.projection.bias.data , 0)\n",
    "\t\tnn.init.constant_(self.decoder.bias.data , 0)\n",
    "\n",
    "\tdef forward(self, batch):\n",
    "\t\tsents = batch['text']\n",
    "\t\tsents, (c_0, h_0) = self.lstm(self.emb(sents))\n",
    "\n",
    "\t\tbs, _, hidden_dim = sents.shape\n",
    "\t\tmax_ents = max([len(x) for x in batch['entity_inds']])\n",
    "\t\t\n",
    "\t\tcont_word_mask = sents.new_zeros(bs, max_ents)\n",
    "\t\tcont_word_embs = sents.new_zeros(bs, max_ents, hidden_dim)\n",
    "\n",
    "\t\tfor b, (sent,entind) in enumerate(zip(sents,batch['entity_inds'])):\n",
    "\t\t\tfor n_ent, wordemb in enumerate([sent[z[0]:z[1]] for z in entind]):\n",
    "\t\t\t\tcont_word_embs[b, n_ent] = torch.mean(wordemb, dim = 0)\n",
    "\t\t\t\tcont_word_mask[b, n_ent] = 1\n",
    "\n",
    "\t\t# bs x max_ents x model_dim\n",
    "\t\tcont_word_embs = self.layer_norm(cont_word_embs)\n",
    "\n",
    "\t\trel1 = self.relation_layer1(cont_word_embs)\n",
    "\t\trel2 = self.relation_layer2(cont_word_embs)\n",
    "\n",
    "\t\t#bs x max_ents x max_ents x model_dim\n",
    "\t\tout = rel1.unsqueeze(1) + rel2.unsqueeze(2)\n",
    "\n",
    "\t\tout = F.relu(self.drop(out))\n",
    "\t\tout = F.relu(self.projection(out))\n",
    "\t\tout = self.decoder(out)\n",
    "\n",
    "\t\tout = out * cont_word_mask.view(bs,max_ents,1,1) * cont_word_mask.view(bs,1,max_ents,1)\n",
    "\n",
    "\t\treturn torch.log_softmax(out, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c5ce5b9604fc8445e4e5f27c24807def7cbbefbcd7dbec7e9c2f61ff534743b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python38564bitbaseconda53d3aa08a74d40a8860ef501a21f2398"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}