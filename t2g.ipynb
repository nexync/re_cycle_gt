{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import tqdm\n",
    "\n",
    "class ModelLSTM(nn.Module):\n",
    "\tdef __init__(self, input_types, relation_types, model_dim, dropout = 0.5):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.word_types = input_types\n",
    "\t\tself.relation_types = relation_types\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.model_dim = model_dim\n",
    "\n",
    "\t\tself.emb = nn.Embedding(input_types, self.model_dim)\n",
    "\t\tself.lstm = nn.LSTM(self.model_dim, self.model_dim//2, batch_first=True, bidirectional=True, num_layers=2)\n",
    "\t\tself.relation_layer1 = nn.Linear(self.model_dim , self.model_dim)\n",
    "\t\tself.relation_layer2 = nn.Linear(self.model_dim , self.model_dim)\n",
    "\t\tself.drop = nn.Dropout(self.dropout)\n",
    "\t\tself.projection = nn.Linear(self.model_dim , self.model_dim)\n",
    "\t\tself.decoder = nn.Linear(self.model_dim , self.relation_types)\n",
    "\t\tself.layer_norm = nn.LayerNorm(self.model_dim)\n",
    "\n",
    "\t\tself.init_params()\n",
    "\n",
    "\tdef init_params(self):\n",
    "\t\tnn.init.xavier_normal_(self.relation_layer1.weight.data)\n",
    "\t\tnn.init.xavier_normal_(self.relation_layer2.weight.data)\n",
    "\t\tnn.init.xavier_normal_(self.projection.weight.data)\n",
    "\t\tnn.init.xavier_normal_(self.decoder.weight.data)\n",
    "\n",
    "\t\tnn.init.constant_(self.relation_layer1.bias.data , 0)\n",
    "\t\tnn.init.constant_(self.relation_layer2.bias.data , 0)\n",
    "\t\tnn.init.constant_(self.projection.bias.data , 0)\n",
    "\t\tnn.init.constant_(self.decoder.bias.data , 0)\n",
    "\n",
    "\tdef forward(self, batch):\n",
    "\t\tsents = batch['text']\n",
    "\t\tsents, (c_0, h_0) = self.lstm(self.emb(sents))\n",
    "\n",
    "\t\tbs, _, hidden_dim = sents.shape\n",
    "\t\tmax_ents = max([len(x) for x in batch['entity_inds']])\n",
    "\t\t\n",
    "\t\tcont_word_mask = sents.new_zeros(bs, max_ents)\n",
    "\t\tcont_word_embs = sents.new_zeros(bs, max_ents, hidden_dim)\n",
    "\n",
    "\t\tfor b, (sent,entind) in enumerate(zip(sents,batch['entity_inds'])):\n",
    "\t\t\tfor n_ent, wordemb in enumerate([sent[z[0]:z[1]] for z in entind]):\n",
    "\t\t\t\tcont_word_embs[b, n_ent] = torch.mean(wordemb, dim = 0)\n",
    "\t\t\t\tcont_word_mask[b, n_ent] = 1\n",
    "\n",
    "\t\t# bs x max_ents x model_dim\n",
    "\t\tcont_word_embs = self.layer_norm(cont_word_embs)\n",
    "\n",
    "\t\trel1 = self.relation_layer1(cont_word_embs)\n",
    "\t\trel2 = self.relation_layer2(cont_word_embs)\n",
    "\n",
    "\t\t#bs x max_ents x max_ents x model_dim\n",
    "\t\tout = rel1.unsqueeze(1) + rel2.unsqueeze(2)\n",
    "\n",
    "\t\tout = self.drop(out)\n",
    "\t\tout = self.projection(out)\n",
    "\t\tout = self.decoder(out)\n",
    "\n",
    "\t\tout = out * cont_word_mask.view(bs,max_ents,1,1) * cont_word_mask.view(bs,1,max_ents,1)\n",
    "\n",
    "\t\treturn torch.log_softmax(out, -1)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, num_relations, dataloader, learning_rate = 1e10, epochs = 30):\n",
    "\t\"\"\"\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Create model\n",
    "\toptimzer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\tcriterion = nn.NLLLoss()\n",
    "\n",
    "\tstate_dict_clone = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "\tfor t in range(epochs):\n",
    "\t\tloss_this_epoch = 0.0\n",
    "\t\tfor batch in tqdm.tqdm(range(len(dataloader))):\n",
    "    \n",
    "\t\t\tlog_probs = model(dataloader[batch])\n",
    "\t\t\tlabels = dataloader[batch]['labels']\t\n",
    "\n",
    "\t\t\tloss = criterion(log_probs.view(-1, num_relations), labels.view(-1))\n",
    "\t\t\tloss_this_epoch += loss.item()\n",
    "\t\t\toptimzer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\t# torch.nn.utils.clip_grad_norm_(\n",
    "\t\t\t#     [p for group in optimzer.param_groups for p in group['params']], CLIP)\n",
    "\t\t\toptimzer.step()\n",
    "\n",
    "\t\t# \t# load best parameters\n",
    "\t\t# curr_state_dict = encdec_model.state_dict()\n",
    "\t\t# for key in state_dict_clone.keys():\n",
    "\t\t# \tcurr_state_dict[key].copy_(state_dict_clone[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_t2g(model, batch):\n",
    "\t\"\"\"\n",
    "\t\n",
    "\t\"\"\"\n",
    "\tprint(batch)\n",
    "\tlog_probs = model(batch)\n",
    "\trelations = torch.argmax(log_probs, -1)\n",
    "\n",
    "\tbs, ne, _ = relations.shape\n",
    "\n",
    "\toutput = [] #list of dictionaries\n",
    "\n",
    "\tfor b in bs:\n",
    "\t\tfor i in range(1, ne):\n",
    "\t\t\tfor j in range(i+1, ne):\n",
    "\t\t\t\t\n",
    "\n",
    "\treturn 0/0\n",
    "\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T2GModel():\n",
    "\tdef __init__(self, vocab):\n",
    "\t\t\n",
    "\t\tself.inp_types = len(vocab.entities.wordlist) + len(vocab.text.wordlist)\n",
    "\t\tself.rel_types = len(vocab.relations.wordlist)\n",
    "\n",
    "\t\tself.t2g = ModelLSTM(self.inp_types, self.rel_types, hidden_dim = 100)\n",
    "\t\tself.vocab = vocab\n",
    "\t\n",
    "\tdef t2g_preprocess(batch):\n",
    "\t\t\"\"\" \n",
    "\t\t\tinput: list of dictionaries in raw_json_format\n",
    "\t\t\toutput: prepreprocessed dictionaries containing text, entity inds\n",
    "\t\t\"\"\"\n",
    "\t\tmodified_dicts = []\n",
    "\t\tfor raw_json_sentence in batch:\n",
    "\n",
    "\t\n",
    "\tdef t2g_eval(batch):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['entity_inds', 'text_lengths', 'entity_lengths', 'text', 'labels'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity_inds': [[(2, 4), (5, 10)], [(1, 3), (7, 12)], [(1, 3), (7, 8)], [(1, 3), (4, 5)], [(1, 3), (8, 9)], [(1, 3), (4, 5)], [(1, 3), (6, 7)], [(4, 6), (7, 8)]], 'text_lengths': [12, 14, 10, 11, 15, 12, 9, 10], 'entity_lengths': [2, 2, 2, 2, 2, 2, 2, 2], 'text': tensor([[   2,    4, 1493, 1497,    6, 1492, 1493, 1494, 1495, 1496,    8,    3,\n",
      "            0,    0,    0],\n",
      "        [   2, 1493, 1497,    9,    4,   10,    6, 1492, 1493, 1494, 1495, 1496,\n",
      "            8,    3,    0],\n",
      "        [   2, 1493, 1497,    9,    4,   10,    6, 1493,    8,    3,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [   2, 1493, 1497,   11, 1498,   12,   13,   14,   15,    8,    3,    0,\n",
      "            0,    0,    0],\n",
      "        [   2, 1493, 1497,   11,   16,   17,   18,    6, 1498,   12,   13,   19,\n",
      "           15,    8,    3],\n",
      "        [   2, 1493, 1497,   11, 1498,   12,   13,    4,   14,   15,    8,    3,\n",
      "            0,    0,    0],\n",
      "        [   2, 1493, 1497,   11,   20,   21, 1499,    8,    3,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [   2,    4,   22,    6, 1493, 1497,   11, 1499,    8,    3,    0,    0,\n",
      "            0,    0,    0]]), 'labels': tensor([[[4, 5],\n",
      "         [5, 4]],\n",
      "\n",
      "        [[4, 5],\n",
      "         [5, 4]],\n",
      "\n",
      "        [[4, 5],\n",
      "         [5, 4]],\n",
      "\n",
      "        [[4, 6],\n",
      "         [6, 4]],\n",
      "\n",
      "        [[4, 6],\n",
      "         [6, 4]],\n",
      "\n",
      "        [[4, 6],\n",
      "         [6, 4]],\n",
      "\n",
      "        [[4, 7],\n",
      "         [7, 4]],\n",
      "\n",
      "        [[4, 7],\n",
      "         [7, 4]]])}\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30036/3947589142.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meval_t2g\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt2g_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30036/3190455188.py\u001b[0m in \u001b[0;36meval_t2g\u001b[1;34m(model, batch)\u001b[0m\n\u001b[0;32m     13\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "eval_t2g(t2g_model, dataloader[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2g_model = ModelLSTM(inp_types, rel_types, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating empty vocabulary object\n",
      "Finished Parsing Text\n",
      "Creating custom dataset for T2G task\n",
      "Creating empty vocabulary object\n",
      "Finished Parsing Text\n",
      "Finished processing raw json file\n"
     ]
    }
   ],
   "source": [
    "# importing the module\n",
    "import json\n",
    "\n",
    "import data_processing as dp\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)\n",
    "\n",
    "vocab = dp.Vocabulary()\n",
    "vocab.parseText(raw_train)\n",
    "\n",
    "dataset = dp.text2GraphDataset(raw_json_file = raw_train)\n",
    "dataloader = dp.getBatches(vocab, dataset, batch_size = 8, shuffle = False)\n",
    "\n",
    "inp_types = len(vocab.entities.wordlist) + len(vocab.text.wordlist)\n",
    "rel_types = len(vocab.relations.wordlist)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c5ce5b9604fc8445e4e5f27c24807def7cbbefbcd7dbec7e9c2f61ff534743b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('recyclegt': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
