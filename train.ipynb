{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38564bitbaseconda53d3aa08a74d40a8860ef501a21f2398",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Global seed set to 42\n"
    }
   ],
   "source": [
    "# import\n",
    "from simplet5 import SimpleT5\n",
    "\n",
    "\n",
    "# instantiate\n",
    "g2t_model = SimpleT5()\n",
    "\n",
    "# load (supports t5, mt5, byT5 models)\n",
    "g2t_model.from_pretrained(\"t5\",\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g2t_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_processing as dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Creating empty vocabulary object\nFinished Parsing Text\nCreating custom dataset for T2G task\nCreating empty vocabulary object\nFinished Parsing Text\nFinished processing raw json file\n"
    }
   ],
   "source": [
    "# importing the module\n",
    "import json\n",
    "  \n",
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)\n",
    "\n",
    "vocab = dp.Vocabulary()\n",
    "vocab.parseText(raw_train)\n",
    "\n",
    "t2g_model = T2GModel\n",
    "\n",
    "dataset = dp.text2GraphDataset(raw_json_file = raw_train)\n",
    "dataloader = dp.getBatches(vocab, dataset, batch_size = 8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_types = len(vocab.entities.wordlist) + len(vocab.text.wordlist)\n",
    "rel_types = len(vocab.relations.wordlist)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelLSTM(nn.Module):\n",
    "\tdef __init__(self, input_types, relation_types, model_dim, dropout = 0.5):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.word_types = input_types\n",
    "\t\tself.relation_types = relation_types\n",
    "\t\tself.dropout = dropout\n",
    "\t\tself.model_dim = model_dim\n",
    "\n",
    "\t\tself.emb = nn.Embedding(input_types, self.model_dim) # 40000 because we use the Bert tokenizer\n",
    "\t\tself.lstm = nn.LSTM(self.model_dim, self.model_dim//2, batch_first=True, bidirectional=True, num_layers=2)\n",
    "\t\tself.relation_layer1 = nn.Linear(self.model_dim , self.model_dim)\n",
    "\t\tself.relation_layer2 = nn.Linear(self.model_dim , self.model_dim)\n",
    "\t\tself.drop = nn.Dropout(self.dropout)\n",
    "\t\tself.projection = nn.Linear(self.model_dim , self.model_dim)\n",
    "\t\tself.decoder = nn.Linear(self.model_dim , self.relation_types)\n",
    "\t\tself.layer_norm = nn.LayerNorm(self.model_dim)\n",
    "\n",
    "\t\tself.init_params()\n",
    "\n",
    "\tdef init_params(self):\n",
    "\t\tnn.init.xavier_normal_(self.relation_layer1.weight.data)\n",
    "\t\tnn.init.xavier_normal_(self.relation_layer2.weight.data)\n",
    "\t\tnn.init.xavier_normal_(self.projection.weight.data)\n",
    "\t\tnn.init.xavier_normal_(self.decoder.weight.data)\n",
    "\n",
    "\t\tnn.init.constant_(self.relation_layer1.bias.data , 0)\n",
    "\t\tnn.init.constant_(self.relation_layer2.bias.data , 0)\n",
    "\t\tnn.init.constant_(self.projection.bias.data , 0)\n",
    "\t\tnn.init.constant_(self.decoder.bias.data , 0)\n",
    "\n",
    "\tdef forward(self, batch):\n",
    "\t\tsents = batch['text']\n",
    "\t\tsents, (c_0, h_0) = self.lstm(self.emb(sents))\n",
    "\n",
    "\t\tbs, _, hidden_dim = sents.shape\n",
    "\t\tmax_ents = max([len(x) for x in batch['entity_inds']])\n",
    "\t\t\n",
    "\t\tcont_word_mask = sents.new_zeros(bs, max_ents)\n",
    "\t\tcont_word_embs = sents.new_zeros(bs, max_ents, hidden_dim)\n",
    "\n",
    "\t\tfor b, (sent,entind) in enumerate(zip(sents,batch['entity_inds'])):\n",
    "\t\t\tfor n_ent, wordemb in enumerate([sent[z[0]:z[1]] for z in entind]):\n",
    "\t\t\t\tcont_word_embs[b, n_ent] = torch.mean(wordemb, dim = 0)\n",
    "\t\t\t\tcont_word_mask[b, n_ent] = 1\n",
    "\n",
    "\t\t# bs x max_ents x model_dim\n",
    "\t\tcont_word_embs = self.layer_norm(cont_word_embs)\n",
    "\n",
    "\t\trel1 = self.relation_layer1(cont_word_embs)\n",
    "\t\trel2 = self.relation_layer2(cont_word_embs)\n",
    "\n",
    "\t\t#bs x max_ents x max_ents x model_dim\n",
    "\t\tout = rel1.unsqueeze(1) + rel2.unsqueeze(2)\n",
    "\n",
    "\t\tout = F.relu(self.drop(out))\n",
    "\t\tout = F.relu(self.projection(out))\n",
    "\t\tout = self.decoder(out)\n",
    "\n",
    "\t\tout = out * cont_word_mask.view(bs,max_ents,1,1) * cont_word_mask.view(bs,1,max_ents,1)\n",
    "\n",
    "\t\treturn torch.log_softmax(out, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([8, 2, 2, 249])"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "t2g_model = ModelLSTM(inp_types, rel_types, 100)\n",
    "\n",
    "#print(dataloader[0])\n",
    "t2g_model.forward(dataloader[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the module\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "  \n",
    "# Opening JSON file\n",
    "f_train = open('json_datasets/train.json', 'r')\n",
    "raw_train = json.load(f_train)\n",
    "f_train.close()\n",
    "\n",
    "f_test = open('json_datasets/test.json', 'r')\n",
    "raw_test = json.load(f_test)\n",
    "f_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeQuotes(lst):\n",
    "    ret = []\n",
    "    for s in lst:\n",
    "        if s != '``' and s != \"''\":\n",
    "            ret.append(s)\n",
    "    return ret\n",
    "\n",
    "def camelCaseSplit(identifier):\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    d = [m.group(0) for m in matches]\n",
    "    new_d = []\n",
    "    for token in d:\n",
    "        token = token.replace('(', '')\n",
    "        token = token.replace(')', '')\n",
    "        token_split = token.split('_')\n",
    "        for t in token_split:\n",
    "            #new_d.append(t.lower())\n",
    "            new_d.append(t)\n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g2tPreprocessSupervised(raw):\n",
    "    df = []\n",
    "    for item in raw:\n",
    "        graph = 'g2t:'\n",
    "        for relation in item['relations']:\n",
    "            graph += ' <H> ' + ' '.join(removeQuotes(relation[0])) + ' <R> '\n",
    "            graph += ' '.join(camelCaseSplit(relation[1])) + ' <T> '\n",
    "            graph += ' '.join(removeQuotes(relation[2]))\n",
    "\n",
    "        ents = [' '.join(removeQuotes(entity)) for entity in item['entities']]\n",
    "        text = item['text']\n",
    "        for i in range(len(ents)):\n",
    "            text = text.replace('<ENT_'+str(i)+'>', ents[i])\n",
    "        sample = [graph, text]\n",
    "        df.append(sample)\n",
    "    return pd.DataFrame(df, columns=['source_text', 'target_text'])\n",
    "\n",
    "def g2tPreprocess(raw):\n",
    "    df = []\n",
    "    graphs = []\n",
    "    entities = []\n",
    "    raw_ents = []\n",
    "    for item in raw:\n",
    "        graph = 'g2t:'\n",
    "        for relation in item['relations']:\n",
    "            graph += ' <H> ' + ' '.join(removeQuotes(relation[0])) + ' <R> '\n",
    "            graph += ' '.join(camelCaseSplit(relation[1])) + ' <T> '\n",
    "            graph += ' '.join(removeQuotes(relation[2]))\n",
    "\n",
    "        ents = [' '.join(removeQuotes(entity)) for entity in item['entities']]\n",
    "        graphs.append(graph)\n",
    "        entities.append(ents)\n",
    "        raw_ents.append(item['entities'])\n",
    "    return graphs, entities, raw_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "source_text  \\\n0     g2t: <H> Abilene Regional Airport <R> city Ser...   \n1     g2t: <H> Abilene Regional Airport <R> city Ser...   \n2     g2t: <H> Adolfo Suárez Madrid–Barajas Airport ...   \n3     g2t: <H> Adolfo Suárez Madrid–Barajas Airport ...   \n4     g2t: <H> Adolfo Suárez Madrid–Barajas Airport ...   \n...                                                 ...   \n4923  g2t: <H> Twilight ( band ) <R> genre <T> Black...   \n4924  g2t: <H> Twilight ( band ) <R> genre <T> Black...   \n4925  g2t: <H> Uruguay <R> leader Name <T> Raúl Fern...   \n4926  g2t: <H> Uruguay <R> leader Name <T> Raúl Fern...   \n4927  g2t: <H> Uruguay <R> leader Name <T> Raúl Fern...   \n\n                                            target_text  \n0     Abilene , Texas is served by the Abilene Regio...  \n1     Abilene Regional Airport serves the city of Ab...  \n2     Adolfo Suárez Madrid–Barajas Airport can be fo...  \n3     Adolfo Suárez Madrid–Barajas Airport is locate...  \n4     Adolfo Suárez Madrid–Barajas Airport is locate...  \n...                                                 ...  \n4923  Aaron Turner plays Electric guitar and played ...  \n4924  Electric guitar Aaron Turner played with Black...  \n4925  Alfredo Zitarrosa died in Montevideo , Uruguay...  \n4926  Alfredo Zitarrosa died in Montevideo , the lea...  \n4927  Raúl Fernando Sendic Rodríguez is the leader o...  \n\n[4928 rows x 2 columns]\n"
    }
   ],
   "source": [
    "train_df = g2tPreprocess(raw_train)\n",
    "test_df = g2tPreprocess(raw_test)\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_g2t(graph, ents, raw_ents, g2t_model):\n",
    "    predText = g2t_model.predict(graph)\n",
    "    for i in range(len(ents)):\n",
    "        if ents[i] in text:\n",
    "            predText.replace(ents[i], \"<ENT_\" + str(i) + \">\")\n",
    "        else:\n",
    "            print(\"WARNING: ENTITY \" + ents[i] + \" NOT FOUND IN PREDICTED TEXT\")\n",
    "    return {'text' : predText, 'entities' : raw_ents}\n",
    "\n",
    "# input: batch of graphs (list of dicts with relations and entities)\n",
    "# output: predicted texts with original entities taken out (list of dicts with text and entities)\n",
    "def predict_g2t(graphs, g2t_model):\n",
    "    pGraphs, ents, raw_ents = g2tPreprocessNoText(graphs) # processed graphs, entities\n",
    "    print(pGraphs)\n",
    "    print(ents)\n",
    "    hyps = [single_g2t(graphs[i], ents[i], raw_ents[i], g2t_model) for i in range(len(graphs))]\n",
    "    # ret = bleu.compute_score(dev_df['target_text'], hyp)\n",
    "    #print(hyp[:10])\n",
    "    return hyps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<ENT_1> serves the city of <ENT_0> .\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([ 2,  5,  9,  4, 10,  6,  7,  8,  3])"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "g2t_sample_out = []\n",
    "for raw in raw_train[0:8]:\n",
    "    g2t = {'text' : raw['text'], 'entities' : raw['entities']}\n",
    "    g2t_sample_out.append(g2t)\n",
    "g2t_sample_out # just text and entities\n",
    "\n",
    "\n",
    "print(g2t_sample_out[1]['text'])\n",
    "dp.text2Indices(vocab, g2t_sample_out[1]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_cycle(text_batch, g2t_model, t2g_model, g2t_opt): # optimizes g2t\n",
    "    t2g_model.eval()\n",
    "    g2t_model.model.train()\n",
    "    with torch.no_grad():\n",
    "        pred_graphs = t2g_model.forward(text_batch)\n",
    "    # syn_batch???\n",
    "    g2t_opt.zero_grad()\n",
    "    pred_text = predict_g2t(pred_graphs, g2t_model)\n",
    "    # convert pred_text to tensor of word indices\n",
    "    loss = F.nll_loss(pred_text.reshape(-1, pred_text.shape[-1]), text_batch.reshape(-1), ignore_index=0) # could be wrong, again\n",
    "    loss.backward()\n",
    "    #nn.utils.clip_grad_norm_(g2t_model.parameters(), config['clip'])\n",
    "    g2t_opt.step()\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_cycle(graph_batch, g2t_model, t2g_model, t2g_opt): # optimizes t2g\n",
    "    g2t_model.model.eval()\n",
    "    t2g_model.train()\n",
    "    with torch.no_grad():\n",
    "        pred_text = predict_g2t(graph_batch, g2t_model)\n",
    "    # convert pred_text to correct format to input into t2g\n",
    "    # syn_batch???\n",
    "    t2g_opt.zero_grad()\n",
    "    pred_graphs = t2g_model.forward(pred_text)\n",
    "    loss = F.nll_loss(pred_graphs.contiguous().view(-1, pred_graphs.shape[-1]), graph_batch.contiguous().view(-1), ignore_index=0) # could be wrong, again\n",
    "    loss.backward()\n",
    "    #nn.utils.clip_grad_norm_(g2t_model.parameters(), config['clip'])\n",
    "    t2g_opt.step()\n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translation(text_batch, graph_batch, g2t_model, t2g_model, g2t_opt, t2g_opt):\n",
    "    loss1 = g_cycle(graph_batch, g2t_model, t2g_model, t2g_opt)\n",
    "    loss2 = t_cycle(text_batch, g2t_model, t2g_model, g2t_opt)\n",
    "    return loss1, loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0%|          | 0/1630 [00:00<?, ?it/s]                                         source_text  \\\n0  g2t: <H> Aarhus Airport <R> city Served <T> Aa...   \n1  g2t: <H> Aarhus Airport <R> city Served <T> Aa...   \n2  g2t: <H> Aarhus Airport <R> city Served <T> Aa...   \n3  g2t: <H> Aarhus Airport <R> elevation Above Th...   \n4  g2t: <H> Aarhus Airport <R> elevation Above Th...   \n5  g2t: <H> Aarhus Airport <R> elevation Above Th...   \n6  g2t: <H> Aarhus Airport <R> location <T> Tirstrup   \n\n                                         target_text  \n0           the Aarhus Airport of Aarhus , Denmark .  \n1  Aarhus Airport serves the city of Aarhus , Den...  \n2         Aarhus Airport serves the city of Aarhus .  \n3    Aarhus Airport is 25.0 metres above sea level .  \n4  Aarhus Airport is at an elevation of 25.0 metr...  \n5  Aarhus Airport is 25.0 metres above the sea le...  \n6            Aarhus Airport is located in Tirstrup .  \n\n"
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2v/tg9q212n2_x1mz9848phgdgr0000gn/T/ipykernel_1819/306416427.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mback_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2t_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2g_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2t_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2g_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/2v/tg9q212n2_x1mz9848phgdgr0000gn/T/ipykernel_1819/306416427.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mgraph_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# needs to be changed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                 \u001b[0mback_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2t_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2g_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2t_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2g_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2v/tg9q212n2_x1mz9848phgdgr0000gn/T/ipykernel_1819/3920010637.py\u001b[0m in \u001b[0;36mback_translation\u001b[0;34m(text_batch, graph_batch, g2t_model, t2g_model, g2t_opt, t2g_opt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mback_translation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2t_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2g_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2t_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2g_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mloss1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2t_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2g_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2g_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2t_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2g_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2t_opt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2v/tg9q212n2_x1mz9848phgdgr0000gn/T/ipykernel_1819/2792169775.py\u001b[0m in \u001b[0;36mg_cycle\u001b[0;34m(graph_batch, g2t_model, t2g_model, t2g_opt)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mt2g_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mpred_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_g2t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2t_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# convert pred_text to correct format to input into t2g\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# syn_batch???\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2v/tg9q212n2_x1mz9848phgdgr0000gn/T/ipykernel_1819/2033847618.py\u001b[0m in \u001b[0;36mpredict_g2t\u001b[0;34m(graphs, g2t_model)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# output: predicted texts with original entities taken out (list of dicts with text and entities)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_g2t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg2t_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mpGraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_ents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg2tPreprocessNoText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraphs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# processed graphs, entities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpGraphs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/2v/tg9q212n2_x1mz9848phgdgr0000gn/T/ipykernel_1819/2897484871.py\u001b[0m in \u001b[0;36mg2tPreprocessNoText\u001b[0;34m(raw)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'g2t:'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mrelation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'relations'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mgraph\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' <H> '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremoveQuotes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' <R> '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mgraph\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcamelCaseSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' <T> '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "def train(epochs):\n",
    "    t2g_opt = torch.optim.Adam(t2g_model.parameters())\n",
    "    g2t_opt = torch.optim.Adam(g2t_model.model.parameters())\n",
    "\n",
    "    for i in range(epochs):\n",
    "        with tqdm.tqdm(dataloader) as tqb:\n",
    "            for j, x in enumerate(tqb):\n",
    "                # need pairings of text/graph batches (unparallel)\n",
    "                text_batch = x\n",
    "                graph_batch = train_df[(j*8):(j*8+7)] # needs to be changed\n",
    "                print(graph_batch)\n",
    "                back_translation(text_batch, graph_batch, g2t_model, t2g_model, g2t_opt, t2g_opt)\n",
    "\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}