{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (t2g.py, line 51)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\Jeffrey\\anaconda3\\envs\\recyclegt\\lib\\site-packages\\IPython\\core\\interactiveshell.py\"\u001b[0m, line \u001b[0;32m3444\u001b[0m, in \u001b[0;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Jeffrey\\AppData\\Local\\Temp/ipykernel_8920/1555699883.py\"\u001b[1;36m, line \u001b[1;32m16\u001b[1;36m, in \u001b[1;35m<module>\u001b[1;36m\u001b[0m\n\u001b[1;33m    import t2g\u001b[0m\n",
      "\u001b[1;36m  File \u001b[1;32m\"c:\\Users\\Jeffrey\\projects\\recylegt\\t2g.py\"\u001b[1;36m, line \u001b[1;32m51\u001b[0m\n\u001b[1;33m    max_ents = max([entity_ind[0] for batch_ent_inds in ent_inds])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import data_processing as dp\n",
    "import json\n",
    "\n",
    "import t2g\n",
    "# import g2t\n",
    "import g2t_copy as g2t\n",
    "\n",
    "# # instantiate models\n",
    "# g2t_model = SimpleT5()\n",
    "# t2g_model = t2g.T2GModel()\n",
    "\n",
    "# # load (supports t5, mt5, byT5 models)\n",
    "# g2t_model.from_pretrained(\"t5\",\"t5-base\")\n",
    "\n",
    "\n",
    "#create dataloader\n",
    "#t, g = dp.create_cycle_dataloader(vocab, batch_size = 8, shuffle=True)\n",
    "\n",
    "\n",
    "class CycleModel():\n",
    "\tdef __init__(self, vocab, device = \"cpu\"):\n",
    "\t\tif device == \"gpu\":\n",
    "\t\t\tself.device = torch.device('cuda:0')\n",
    "\t\telse:\n",
    "\t\t\tself.device = torch.device('cpu')\n",
    "\n",
    "\t\tself.t2g_model = t2g.T2GModel(vocab, self.device, 768)\n",
    "\t\tself.g2t_model = g2t.G2TModel(vocab)\n",
    "\t\tself.t2g_opt = torch.optim.Adam(self.t2g_model.model.parameters())\n",
    "\t\tself.g2t_opt = torch.optim.Adam(self.g2t_model.t5_model.parameters())\n",
    "\t\tself.vocab = vocab\n",
    "    \n",
    "\tdef t_cycle(self, text_batch): # optimizes g2t\n",
    "\t\tself.t2g_model.eval()\n",
    "\t\tself.g2t_model.train()\n",
    "\n",
    "\t\tgold_text = self.g2t_model.g2t_preprocess(text_batch, mode=\"TGT\")\n",
    "\t\tprint(gold_text)\n",
    "\t\t# gold_text = gold_text.to(self.device) # bs x gold_text_len\n",
    "\t\t# bs, gold_text_len = gold_text.shape\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred_graphs = self.t2g_model.predict(text_batch) #synthetic batch of graphs\n",
    "\t\t\n",
    "\t\tself.g2t_opt.zero_grad()\n",
    "\n",
    "\t\t# #tokenize pred_graphs\n",
    "\t\tpred_graphs, ents, raw_ents = self.g2t_model.g2t_preprocess(pred_graphs, mode=\"G2T\")\n",
    "\t\tpred_graphs_ids = self.g2t_model.tokenizer(pred_graphs, return_tensors='pt', truncation=True, padding=True).input_ids\n",
    "\t\tgold_text_ids = self.g2t_model.tokenizer(gold_text, return_tensors='pt', truncation=True, padding=True).input_ids\n",
    "\t\tloss = self.g2t_model.t5_model(input_ids = pred_graphs_ids, labels = gold_text_ids).loss\n",
    "\t\tloss.backward()\n",
    "\t\tself.g2t_opt.step()\n",
    "\t\treturn loss.item()\n",
    "\n",
    "\n",
    "\tdef g_cycle(self, graph_batch): # optimizes t2g\n",
    "\t\t\"\"\"\n",
    "\t\t\tInput: graph_batch: list (length batch_size) of dicts with entities and relations\n",
    "\n",
    "\t\t\tPerforms G2T then optimizes T2G by computing loss of generated graph and original (gold) graph\n",
    "\t\t\"\"\"\n",
    "\t\tself.g2t_model.eval()\n",
    "\t\tself.t2g_model.train()\n",
    "\t\tmax_ents = max([len(graph[\"entities\"]) for graph in graph_batch])\n",
    "\t\tgold_graphs = [dp.relation2Indices(self.vocab, graph, max_ents) for graph in graph_batch]\n",
    "\t\tgold_graphs = torch.stack(gold_graphs)\n",
    "\t\tgold_graphs = gold_graphs.to(self.device) # bs x max_ents x max_ents - used for loss computation\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred_text = self.g2t_model.predict(graph_batch)\n",
    "\t\t#print(gold_graphs[0])\n",
    "\t\t#print(pred_text[0])\n",
    "\n",
    "\t\tself.t2g_opt.zero_grad()\n",
    "\t\t\n",
    "\t\tpred_text, pred_text_ents = self.t2g_model.t2g_preprocess(pred_text)\n",
    "\n",
    "\t\tgraph_log_probs = self.t2g_model.model.forward(pred_text.to(self.device), pred_text_ents.to(self.device)) # bs x max_ents x max_ents x num_relations - log probs of each relation between all entities in each batch\n",
    "\t\t\n",
    "\t\tloss = F.nll_loss(graph_log_probs.view(-1, graph_log_probs.shape[-1]), gold_graphs.view(-1), ignore_index=self.vocab.relations.word2idx['<EMPTY>']) # ignore index should be 0\n",
    "\t\tloss.backward()\n",
    "\t\t#nn.utils.clip_grad_norm_(g2t_model.parameters(), config['clip'])\n",
    "\t\tself.t2g_opt.step()\n",
    "\t\treturn loss.item()\n",
    "\n",
    "\tdef back_translation(self, text_batch, graph_batch):\n",
    "\t\tg_loss = self.g_cycle(graph_batch)\n",
    "\t\tt_loss = self.t_cycle(text_batch)\n",
    "\t\treturn g_loss, t_loss\n",
    "\n",
    "\tdef train(self, epochs, batch_size, learning_rate, shuffle):\n",
    "\n",
    "\t\tfor i in range(epochs):\n",
    "\t\t\ttcycle_dataloader, gcycle_dataloader = dp.create_cycle_dataloader(raw_json_file=self.vocab.raw_data, batch_size = batch_size, shuffle=shuffle)\n",
    "\t\t\tdataloader = list(zip(tcycle_dataloader, gcycle_dataloader))\n",
    "\t\t\tfor index, (tbatch, gbatch) in tqdm.tqdm(enumerate(dataloader)):\n",
    "\t\t\t\tg_loss, t_loss = self.back_translation(tbatch, gbatch)\n",
    "                    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating empty vocabulary object\n",
      "Finished Parsing Text\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/2v/tg9q212n2_x1mz9848phgdgr0000gn/T/ipykernel_19455/2774105374.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mtcycle_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcycle_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_cycle_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rain' is not defined"
     ]
    }
   ],
   "source": [
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)\n",
    "\n",
    "vocab = dp.Vocabulary()\n",
    "vocab.parseText(raw_train[0:16])\n",
    "\n",
    "#create cycle\n",
    "\n",
    "cycle_model = CycleModel(vocab)\n",
    "loader = dp.create_cycle_dataloader(vocab.raw_data, batch_size = 8, shuffle=False)\n",
    "\n",
    "tbatch = tcycle_dataloader[0]\n",
    "gbatch = gcycle_dataloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'relations': [[['Aarhus', 'Airport'], 'cityServed', ['``', 'Aarhus', ',', 'Denmark', \"''\"]]], 'text': 'the <ENT_1> of <ENT_0> .', 'entities': [['``', 'Aarhus', ',', 'Denmark', \"''\"], ['Aarhus', 'Airport']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'cityServed', ['``', 'Aarhus', ',', 'Denmark', \"''\"]]], 'text': '<ENT_1> serves the city of <ENT_0> .', 'entities': [['``', 'Aarhus', ',', 'Denmark', \"''\"], ['Aarhus', 'Airport']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'cityServed', ['Aarhus']]], 'text': '<ENT_1> serves the city of <ENT_0> .', 'entities': [['Aarhus'], ['Aarhus', 'Airport']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'elevationAboveTheSeaLevel_(in_metres)', ['25.0']]], 'text': '<ENT_0> is <ENT_1> metres above sea level .', 'entities': [['Aarhus', 'Airport'], ['25.0']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'elevationAboveTheSeaLevel_(in_metres)', ['25.0']]], 'text': '<ENT_0> is at an elevation of <ENT_1> metres above seal level .', 'entities': [['Aarhus', 'Airport'], ['25.0']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'elevationAboveTheSeaLevel_(in_metres)', ['25.0']]], 'text': '<ENT_0> is <ENT_1> metres above the sea level .', 'entities': [['Aarhus', 'Airport'], ['25.0']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'location', ['Tirstrup']]], 'text': '<ENT_0> is located in <ENT_1> .', 'entities': [['Aarhus', 'Airport'], ['Tirstrup']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'location', ['Tirstrup']]], 'text': 'the location of <ENT_0> is <ENT_1> .', 'entities': [['Aarhus', 'Airport'], ['Tirstrup']]}]\n"
     ]
    }
   ],
   "source": [
    "print(tbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the Aarhus Airport of Aarhus , Denmark .', 'Aarhus Airport serves the city of Aarhus , Denmark .', 'Aarhus Airport serves the city of Aarhus .', 'Aarhus Airport is 25.0 metres above sea level .', 'Aarhus Airport is at an elevation of 25.0 metres above seal level .', 'Aarhus Airport is 25.0 metres above the sea level .', 'Aarhus Airport is located in Tirstrup .', 'the location of Aarhus Airport is Tirstrup .']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2224383354187012"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycle_model.t_cycle(tbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<g2t_copy.G2TModel at 0x1039517f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycle_model.g2t_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: ENTITY `` Aarhus Lufthavn A/S '' NOT FOUND IN PREDICTED TEXT. APPENDING TO THE END OF TEXT\n",
      "WARNING: ENTITY `` Aarhus Lufthavn A/S '' NOT FOUND IN PREDICTED TEXT. APPENDING TO THE END OF TEXT\n",
      "WARNING: ENTITY Aktieselskab NOT FOUND IN PREDICTED TEXT. APPENDING TO THE END OF TEXT\n",
      "WARNING: ENTITY Aktieselskab NOT FOUND IN PREDICTED TEXT. APPENDING TO THE END OF TEXT\n",
      "WARNING: ENTITY Aktieselskab NOT FOUND IN PREDICTED TEXT. APPENDING TO THE END OF TEXT\n",
      "WARNING: ENTITY 2776.0 NOT FOUND IN PREDICTED TEXT. APPENDING TO THE END OF TEXT\n",
      "WARNING: ENTITY 2776.0 NOT FOUND IN PREDICTED TEXT. APPENDING TO THE END OF TEXT\n",
      "WARNING: ENTITY 2776.0 NOT FOUND IN PREDICTED TEXT. APPENDING TO THE END OF TEXT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.810157775878906"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycle_model.g_cycle(gbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'relations': [[['Aarhus', 'Airport'], 'cityServed', ['``', 'Aarhus', ',', 'Denmark', \"''\"]]], 'text': 'the <ENT_1> of <ENT_0> .', 'entities': [['``', 'Aarhus', ',', 'Denmark', \"''\"], ['Aarhus', 'Airport']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'cityServed', ['``', 'Aarhus', ',', 'Denmark', \"''\"]]], 'text': '<ENT_1> serves the city of <ENT_0> .', 'entities': [['``', 'Aarhus', ',', 'Denmark', \"''\"], ['Aarhus', 'Airport']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'cityServed', ['Aarhus']]], 'text': '<ENT_1> serves the city of <ENT_0> .', 'entities': [['Aarhus'], ['Aarhus', 'Airport']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'elevationAboveTheSeaLevel_(in_metres)', ['25.0']]], 'text': '<ENT_0> is <ENT_1> metres above sea level .', 'entities': [['Aarhus', 'Airport'], ['25.0']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'elevationAboveTheSeaLevel_(in_metres)', ['25.0']]], 'text': '<ENT_0> is at an elevation of <ENT_1> metres above seal level .', 'entities': [['Aarhus', 'Airport'], ['25.0']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'elevationAboveTheSeaLevel_(in_metres)', ['25.0']]], 'text': '<ENT_0> is <ENT_1> metres above the sea level .', 'entities': [['Aarhus', 'Airport'], ['25.0']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'location', ['Tirstrup']]], 'text': '<ENT_0> is located in <ENT_1> .', 'entities': [['Aarhus', 'Airport'], ['Tirstrup']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'location', ['Tirstrup']]], 'text': 'the location of <ENT_0> is <ENT_1> .', 'entities': [['Aarhus', 'Airport'], ['Tirstrup']]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = list(zip(tcycle_dataloader, gcycle_dataloader))\n",
    "for index, (tbatch, gbatch) in tqdm.tqdm(enumerate(diataloader)):\n",
    "\tprint(tbatch)\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c5ce5b9604fc8445e4e5f27c24807def7cbbefbcd7dbec7e9c2f61ff534743b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
