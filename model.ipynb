{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import data_processing as dp\n",
    "import json\n",
    "\n",
    "import t2g\n",
    "# import g2t\n",
    "import g2t_copy as g2t\n",
    "\n",
    "# # instantiate models\n",
    "# g2t_model = SimpleT5()\n",
    "# t2g_model = t2g.T2GModel()\n",
    "\n",
    "# # load (supports t5, mt5, byT5 models)\n",
    "# g2t_model.from_pretrained(\"t5\",\"t5-base\")\n",
    "\n",
    "\n",
    "#create dataloader\n",
    "#t, g = dp.create_cycle_dataloader(vocab, batch_size = 8, shuffle=True)\n",
    "\n",
    "\n",
    "class CycleModel():\n",
    "\tdef __init__(self, vocab, device = \"cpu\"):\n",
    "\t\tif device == \"gpu\":\n",
    "\t\t\tself.device = torch.device('cuda:0')\n",
    "\t\telse:\n",
    "\t\t\tself.device = torch.device('cpu')\n",
    "\n",
    "\t\tself.t2g_model = t2g.T2GModel(vocab, self.device, 768)\n",
    "\t\tself.g2t_model = g2t.G2TModel(vocab)\n",
    "\t\tself.t2g_opt = torch.optim.Adam(self.t2g_model.model.parameters())\n",
    "\t\tself.g2t_opt = torch.optim.Adam(self.g2t_model.t5_model.model.parameters())\n",
    "\t\tself.vocab = vocab\n",
    "    \n",
    "\tdef t_cycle(self, text_batch): # optimizes g2t\n",
    "\t\tself.t2g_model.eval()\n",
    "\t\tself.g2t_model.train()\n",
    "\n",
    "\t\tgold_text, _ = self.t2g_model.t2g_preprocess(text_batch, mode = \"TGT\").to(self.device) # bs x gold_text_len\n",
    "\t\tbs, gold_text_len = gold_text.shape\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred_graphs = self.t2g_model.predict(text_batch) #synthetic batch of graphs\n",
    "\t\t\n",
    "\t\tself.g2t_opt.zero_grad()\n",
    "\n",
    "\t\t# #tokenize pred_graphs\n",
    "\t\tpred_graphs, ents, raw_ents = self.g2t_model.g2t_preprocess(pred_graphs)\n",
    "\t\tpred_graphs_ids = self.g2t_model.tokenizer(pred_graphs, return_tensors='pt').input_ids\n",
    "\t\tgold_text_ids = self.g2t_model.tokenizer(gold_text, return_tensors='pt').input_ids\n",
    "\t\tloss = self.g2t_model(input_ids = pred_graphs_ids, labels = gold_text_ids).loss\n",
    "\t\tloss.backward()\n",
    "\t\tself.g2t_opt.step()\n",
    "\n",
    "\t\t# # CODE IS UNNECESSARY IF WE ARE BLACKBOXING LOSS FUNCTION - DAVID's CODE HERE\n",
    "\n",
    "\t\t# text_log_probs = self.g2t_model.t5_model.model.forward(pred_graphs) # bs x out_text_len x vocab_size need to check implementation of forward\n",
    "\t\t# _, out_text_len, vocab_size = text_log_probs.shape\n",
    "\t\t# #pred_text = self.g2t_model.predict(pred_graphs)   #note: this would not be predict here - it would be calling running through the model i think\n",
    "\t\t\n",
    "\n",
    "\t\t# if gold_text_len < out_text_len:\n",
    "\t\t# \tempties = torch.ones(bs, out_text_len - gold_text_len) * self.vocab.text.word2idx[\"<EMPTY>\"] \n",
    "\t\t# \tgold_text = torch.cat((gold_text, empties), dim = 1)\n",
    "\t\t# elif gold_text_len > out_text_len: # need to double check\n",
    "\t\t# \tempty_probs = torch.ones(bs, gold_text_len - out_text_len, vocab_size) * float('-inf')\n",
    "\t\t# \tempty_probs[:, :, self.vocab.text.word2idx[\"<EMPTY>\"]] = 0\n",
    "\t\t# \ttext_log_probs = torch.cat((text_log_probs, empty_probs), dim = 1)\n",
    "\n",
    "\t\t# assert gold_text.shape[1] == text_log_probs.shape[1], \"Gold mismatches text log probs length in T cycle\"\n",
    "\t\t# # convert pred_text to tensor of word indices\n",
    "\t\t# loss = F.nll_loss(text_log_probs.view(-1, text_log_probs.shape[-1]), gold_text.view(-1), ignore_index=0) # could be wrong, again\n",
    "\t\t# loss.backward()\n",
    "\t\t# #nn.utils.clip_grad_norm_(g2t_model.parameters(), config['clip'])\n",
    "\t\t# self.g2t_opt.step()\n",
    "\t\treturn loss.item()\n",
    "\n",
    "\n",
    "\n",
    "\tdef g_cycle(self, graph_batch): # optimizes t2g\n",
    "\t\t\"\"\"\n",
    "\t\t\tInput: graph_batch: list (length batch_size) of dicts with entities and relations\n",
    "\n",
    "\t\t\tPerforms G2T then optimizes T2G by computing loss of generated graph and original (gold) graph\n",
    "\t\t\"\"\"\n",
    "\t\tself.g2t_model.eval()\n",
    "\t\tself.t2g_model.train()\n",
    "\t\tmax_ents = max([len(graph[\"entities\"]) for graph in graph_batch])\n",
    "\t\tgold_graphs = [dp.relation2Indices(self.vocab, graph, max_ents) for graph in graph_batch]\n",
    "\t\tgold_graphs = torch.LongTensor(gold_graphs).to(self.device) # bs x max_ents x max_ents - used for loss computation\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred_text = self.g2t_model.predict(graph_batch)\n",
    "\n",
    "\t\tself.t2g_opt.zero_grad()\n",
    "\t\tpred_text, pred_text_ents = self.t2g_model.t2g_preprocess(pred_text)\n",
    "\n",
    "\t\tgraph_log_probs = self.t2g_model.model.forward(pred_text.to(self.device), pred_text_ents.to(self.device)) # bs x max_ents x max_ents x num_relations - log probs of each relation between all entities in each batch\n",
    "\t\tloss = F.nll_loss(graph_log_probs.view(-1, graph_log_probs.shape[-1]), gold_graphs.view(-1), ignore_index=self.vocab.relations.word2idx['<EMPTY>']) # ignore index should be 0\n",
    "\t\tloss.backward()\n",
    "\t\t#nn.utils.clip_grad_norm_(g2t_model.parameters(), config['clip'])\n",
    "\t\tself.t2g_opt.step()\n",
    "\t\treturn loss.item()\n",
    "\n",
    "\tdef back_translation(self, text_batch, graph_batch):\n",
    "\t\tg_loss = self.g_cycle(graph_batch)\n",
    "\t\tt_loss = self.t_cycle(text_batch)\n",
    "\t\treturn g_loss, t_loss\n",
    "\n",
    "\tdef train(self, epochs, batch_size, learning_rate, shuffle):\n",
    "\n",
    "\t\tfor i in range(epochs):\n",
    "\t\t\ttcycle_dataloader, gcycle_dataloader = dp.create_cycle_dataloader(raw_json_file=self.vocab.raw_data, batch_size = batch_size, shuffle=shuffle)\n",
    "\t\t\tdataloader = list(zip(tcycle_dataloader, gcycle_dataloader))\n",
    "\t\t\tfor index, (tbatch, gbatch) in tqdm.tqdm(enumerate(dataloader)):\n",
    "\t\t\t\tg_loss, t_loss = self.back_translation(tbatch, gbatch)\n",
    "                    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating empty vocabulary object\n",
      "Finished Parsing Text\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 242M/242M [00:07<00:00, 32.9MB/s] \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'T5ForConditionalGeneration' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12496/2128596489.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m#create cycle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mcycle_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCycleModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtcycle_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgcycle_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_cycle_dataloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12496/3481848411.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, vocab, device)\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg2t_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg2t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mG2TModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt2g_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt2g_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg2t_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg2t_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt5_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\recyclegt\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'T5ForConditionalGeneration' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)\n",
    "\n",
    "vocab = dp.Vocabulary()\n",
    "vocab.parseText(raw_train)\n",
    "\n",
    "#create cycle\n",
    "\n",
    "cycle_model = CycleModel(vocab)\n",
    "\n",
    "tcycle_dataloader, gcycle_dataloader = dp.create_cycle_dataloader(vocab.raw_data, batch_size = 8, shuffle=False)\n",
    "\n",
    "tbatch = tcycle_dataloader[0]\n",
    "gbatch = gcycle_dataloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cycle_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12496/3372235925.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcycle_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mg2t_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cycle_model' is not defined"
     ]
    }
   ],
   "source": [
    "cycle_model.g2t_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'relations': [[['Aarhus', 'Airport'], 'cityServed', ['``', 'Aarhus', ',', 'Denmark', \"''\"]]], 'text': 'the <ENT_1> of <ENT_0> .', 'entities': [['``', 'Aarhus', ',', 'Denmark', \"''\"], ['Aarhus', 'Airport']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'cityServed', ['``', 'Aarhus', ',', 'Denmark', \"''\"]]], 'text': '<ENT_1> serves the city of <ENT_0> .', 'entities': [['``', 'Aarhus', ',', 'Denmark', \"''\"], ['Aarhus', 'Airport']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'cityServed', ['Aarhus']]], 'text': '<ENT_1> serves the city of <ENT_0> .', 'entities': [['Aarhus'], ['Aarhus', 'Airport']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'elevationAboveTheSeaLevel_(in_metres)', ['25.0']]], 'text': '<ENT_0> is <ENT_1> metres above sea level .', 'entities': [['Aarhus', 'Airport'], ['25.0']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'elevationAboveTheSeaLevel_(in_metres)', ['25.0']]], 'text': '<ENT_0> is at an elevation of <ENT_1> metres above seal level .', 'entities': [['Aarhus', 'Airport'], ['25.0']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'elevationAboveTheSeaLevel_(in_metres)', ['25.0']]], 'text': '<ENT_0> is <ENT_1> metres above the sea level .', 'entities': [['Aarhus', 'Airport'], ['25.0']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'location', ['Tirstrup']]], 'text': '<ENT_0> is located in <ENT_1> .', 'entities': [['Aarhus', 'Airport'], ['Tirstrup']]}\n",
      " {'relations': [[['Aarhus', 'Airport'], 'location', ['Tirstrup']]], 'text': 'the location of <ENT_0> is <ENT_1> .', 'entities': [['Aarhus', 'Airport'], ['Tirstrup']]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = list(zip(tcycle_dataloader, gcycle_dataloader))\n",
    "for index, (tbatch, gbatch) in tqdm.tqdm(enumerate(dataloader)):\n",
    "\tprint(tbatch)\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c5ce5b9604fc8445e4e5f27c24807def7cbbefbcd7dbec7e9c2f61ff534743b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('recyclegt': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
