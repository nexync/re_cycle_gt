{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import data_processing as dp\n",
    "import json\n",
    "\n",
    "import t2g\n",
    "# import g2t\n",
    "import g2t_copy as g2t\n",
    "\n",
    "# # instantiate models\n",
    "# g2t_model = SimpleT5()\n",
    "# t2g_model = t2g.T2GModel()\n",
    "\n",
    "# # load (supports t5, mt5, byT5 models)\n",
    "# g2t_model.from_pretrained(\"t5\",\"t5-base\")\n",
    "\n",
    "\n",
    "#create dataloader\n",
    "#t, g = dp.create_cycle_dataloader(vocab, batch_size = 8, shuffle=True)\n",
    "\n",
    "\n",
    "class CycleModel():\n",
    "\tdef __init__(self, vocab, device = \"cpu\"):\n",
    "\t\tif device == \"gpu\":\n",
    "\t\t\tself.device = torch.device('cuda:0')\n",
    "\t\telse:\n",
    "\t\t\tself.device = torch.device('cpu')\n",
    "\n",
    "\t\tself.t2g_model = t2g.T2GModel(vocab, self.device, 768)\n",
    "\t\tself.g2t_model = g2t.G2TModel(vocab)\n",
    "\t\tself.t2g_opt = torch.optim.Adam(self.t2g_model.model.parameters())\n",
    "\t\tself.g2t_opt = torch.optim.Adam(self.g2t_model.t5_model.parameters())\n",
    "\t\tself.vocab = vocab\n",
    "    \n",
    "\tdef t_cycle(self, text_batch): # optimizes g2t\n",
    "\t\tself.t2g_model.eval()\n",
    "\t\tself.g2t_model.train()\n",
    "\n",
    "\t\tgold_text = self.g2t_model.g2t_preprocess(text_batch, mode=\"TGT\")\n",
    "\t\tprint(gold_text)\n",
    "\t\t# gold_text = gold_text.to(self.device) # bs x gold_text_len\n",
    "\t\t# bs, gold_text_len = gold_text.shape\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred_graphs = self.t2g_model.predict(text_batch) #synthetic batch of graphs\n",
    "\t\t\n",
    "\t\tself.g2t_opt.zero_grad()\n",
    "\n",
    "\t\t# #tokenize pred_graphs\n",
    "\t\tpred_graphs, ents, raw_ents = self.g2t_model.g2t_preprocess(pred_graphs, mode=\"G2T\")\n",
    "\t\tpred_graphs_ids = self.g2t_model.tokenizer(pred_graphs, return_tensors='pt', truncation=True, padding=True).input_ids\n",
    "\t\tgold_text_ids = self.g2t_model.tokenizer(gold_text, return_tensors='pt', truncation=True, padding=True).input_ids\n",
    "\t\tloss = self.g2t_model.t5_model(input_ids = pred_graphs_ids, labels = gold_text_ids).loss\n",
    "\t\tloss.backward()\n",
    "\t\tself.g2t_opt.step()\n",
    "\t\treturn loss.item()\n",
    "\n",
    "\n",
    "\tdef g_cycle(self, graph_batch): # optimizes t2g\n",
    "\t\t\"\"\"\n",
    "\t\t\tInput: graph_batch: list (length batch_size) of dicts with entities and relations\n",
    "\n",
    "\t\t\tPerforms G2T then optimizes T2G by computing loss of generated graph and original (gold) graph\n",
    "\t\t\"\"\"\n",
    "\t\tself.g2t_model.eval()\n",
    "\t\tself.t2g_model.train()\n",
    "\t\tmax_ents = max([len(graph[\"entities\"]) for graph in graph_batch])\n",
    "\t\tgold_graphs = [dp.relation2Indices(self.vocab, graph, max_ents) for graph in graph_batch]\n",
    "\t\tgold_graphs = torch.stack(gold_graphs)\n",
    "\t\tgold_graphs = gold_graphs.to(self.device) # bs x max_ents x max_ents - used for loss computation\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tpred_text = self.g2t_model.predict(graph_batch)\n",
    "\t\t#print(gold_graphs[0])\n",
    "\t\t#print(pred_text[0])\n",
    "\n",
    "\t\tself.t2g_opt.zero_grad()\n",
    "\t\t\n",
    "\t\tpred_text, pred_text_ents = self.t2g_model.t2g_preprocess(pred_text)\n",
    "\n",
    "\t\tgraph_log_probs = self.t2g_model.model.forward(pred_text.to(self.device), pred_text_ents.to(self.device)) # bs x max_ents x max_ents x num_relations - log probs of each relation between all entities in each batch\n",
    "\t\t\n",
    "\t\tloss = F.nll_loss(graph_log_probs.view(-1, graph_log_probs.shape[-1]), gold_graphs.view(-1), ignore_index=self.vocab.relations.word2idx['<EMPTY>']) # ignore index should be 0\n",
    "\t\tloss.backward()\n",
    "\t\t#nn.utils.clip_grad_norm_(g2t_model.parameters(), config['clip'])\n",
    "\t\tself.t2g_opt.step()\n",
    "\t\treturn loss.item()\n",
    "\n",
    "\tdef back_translation(self, text_batch, graph_batch):\n",
    "\t\tg_loss = self.g_cycle(graph_batch)\n",
    "\t\tt_loss = self.t_cycle(text_batch)\n",
    "\t\treturn g_loss, t_loss\n",
    "\n",
    "\tdef train(self, epochs, batch_size, learning_rate, shuffle):\n",
    "\n",
    "\t\tfor i in range(epochs):\n",
    "\t\t\ttcycle_dataloader, gcycle_dataloader = dp.create_cycle_dataloader(raw_json_file=self.vocab.raw_data, batch_size = batch_size, shuffle=shuffle)\n",
    "\t\t\tdataloader = list(zip(tcycle_dataloader, gcycle_dataloader))\n",
    "\t\t\tfor index, (tbatch, gbatch) in tqdm.tqdm(enumerate(dataloader)):\n",
    "\t\t\t\tg_loss, t_loss = self.back_translation(tbatch, gbatch)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('json_datasets/train.json', 'r')\n",
    "\n",
    "raw_train = json.load(f)\n",
    "\n",
    "vocab = dp.Vocabulary()\n",
    "vocab.parseText(raw_train[0:16])\n",
    "\n",
    "#create cycle\n",
    "\n",
    "cycle_model = CycleModel(vocab)\n",
    "tcycle_dataloader, gcycle_dataloader = dp.create_cycle_dataloader(vocab.raw_data, batch_size = 8, shuffle=False)\n",
    "\n",
    "tbatch = tcycle_dataloader[0]\n",
    "gbatch = gcycle_dataloader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9c5ce5b9604fc8445e4e5f27c24807def7cbbefbcd7dbec7e9c2f61ff534743b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('recyclegt': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
